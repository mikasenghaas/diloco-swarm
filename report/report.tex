\documentclass{article}

\usepackage[accepted]{icml2023}

% Packages
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath, amssymb}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{float}
\usepackage{tcolorbox}
\usepackage{algorithm, algpseudocodex}
\usepackage{epstopdf}
\usepackage[colorlinks=true,
            linkcolor=blue,
            filecolor=blue,
            urlcolor=blue,
            citecolor=blue]{hyperref}

% Graphics
\DeclareGraphicsExtensions{.png,.pdf}

% Aliases
\newcommand{\github}{\href{https://github.com/mikasenghaas/diloco-swarm}{GitHub}}
\newcommand{\wandb}{\href{https://wandb.ai/mikasenghaas/diloco-swarm}{Weights \& Biases}}
\newcommand{\gist}{\href{https://gist.github.com/mikasenghaas/5fa1aa77ea69f187f531a5889983c249}{GitHub Gist}}

% Colors
\definecolor{bblue}{HTML}{5884E2}
\definecolor{oorange}{HTML}{F19E38}
\definecolor{ppurple}{HTML}{9900FF}

% Colored boxes
\newcommand{\orangebox}{\colorbox{oorange!50}{\hspace{0.3em}}}
\newcommand{\bluecircle}{\textcolor{bblue}{\LARGE$\bullet$}}
\newcommand{\purplecircle}{\textcolor{ppurple}{\LARGE$\bullet$}}

% Code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Running Title
\icmltitlerunning{DiLoCo-SWARM}

\begin{document}

\begin{titlepage}
\begin{center}
    \Large{\textsc{École Polytechnique Fédérale de Lausanne}}\\
    \vspace{1cm}
    \includegraphics[width=5cm]{figures/epfl.png}
    \vspace{1cm}
    
    \large{Optional Master Research Project, \textit{January 2025}}\\
    
    \vspace{1cm}
    \rule{\textwidth}{1pt}\vspace{15pt}
    \Huge{DiLoCo-SWARM}
    \rule{\textwidth}{1pt}
    
    \vspace{1cm}
    
    \large{\textsc{Mika Senghaas}\\\texttt{mika.senghaas@epfl.ch}}
    \vspace{\stretch{1}}
    
    \large{
      \textit{Supervisors}\\
      \textsc{Martijn De Vos}, \textsc{Akash Dhasade}, \textsc{Rishi Sharma}}\\
      \vspace{0.5cm}
      \textit{Professor}\\
      \textsc{Anne-Marie Kermarrec}\\
      \vspace{0.5cm}
      \large{\textit{Laboratory}\\
      Scalable Computing Systems (SaCS)\\
    }
\end{center}
\end{titlepage}

% Title
% \twocolumn[
%   \icmltitle{DiLoCo-SWARM}
%   \begin{icmlauthorlist}
%     \icmlauthor{Mika Senghaas}{author}
%     \icmlauthor{Martijn De Vos}{supervisor}
%     \icmlauthor{Akash Dhasad}{supervisor}
%     \icmlauthor{Rishi Sharma}{supervisor}
%   \end{icmlauthorlist}
%   \icmlaffiliation{author}{Author}
%   \icmlaffiliation{supervisor}{Supervisor}
%   \icmlcorrespondingauthor{Mika Senghaas}{mika.senghaas@epfl.ch}
%   \icmlkeywords{Distributed Training, Decentralized AI, SWARM, DiLoCo}
%   \vskip 0.3in
% ]
% \printAffiliationsAndNotice{}

% Abstract
\begin{abstract}
  We investigate DiLoCo-SWARM, a decentralized training approach that combines
  the pipeline and data parallelism of SWARM with DiLoCo's reduced-frequency
  gradient synchronization. Our experiments on language modeling tasks show that
  DiLoCo-SWARM matches or surpasses fully synchronized SWARM baselines despite
  synchronizing gradients up to 50 times less frequently.
\end{abstract}

\section{Introduction}

Modern foundation models contain billions of parameters and are trained on trillions of tokens~\cite{chowdhery2022palm,brown2023gpt3,dubey2024llama3,google2024gemini}. Achieving this scale necessitates orchestrating thousands of GPUs to distribute computation~\cite{dubey2024llama3,deepseekai2024}. However, conventional parallelization techniques rely heavily on high-bandwidth interconnects to avoid communication bottlenecks. As a result, cutting-edge models are primarily trained in high-performance clusters (HPCs). Operating these clusters incurs significant costs, thereby restricting research abilities to a select group of corporate and state actors~\cite{jaghouar2024intellect1}.

Decentralized training has emerged as a promising alternative. By leveraging low-cost, globally distributed compute resources, decentralized training promises to democratize access to model development. However, this paradigm shift introduces substantial technical challenges, particularly in accommodating latency and bandwidth limitations in Internet connections, heterogeneous compute environments and dynamic node participation.

While decentralized training efforts cannot yet match the 
scale and efficiency of centralized training, the field 
has made significant progress in recent years. INTELLECT-1~\cite{jaghouar2024intellect1}, a 10B parameter model trained collaboratively across the globe, demonstrates this most recently. The model builds on DiLoCo~\cite{douillard2023diloco}, a data parallel method that reduces communication overhead by synchronizing gradients infrequently through a dual optimization scheme. DiLoCo achieves competitive performance with traditional data parallelization but only synchronizes gradients every 500 steps instead of after each step. However, DiLoCo requires each node to store a full model replica, limiting participation to high-performance hardware.

SWARM~\cite{ryabinin2023swarm} offers a promising, though less explored, alternative for larger-scale decentralized training. SWARM combines both data and pipeline parallelism and introduces several novel techniques to enable fault-tolerant training in heterogeneous compute and network conditions. By partitioning the model across nodes, SWARM can train larger models and allow devices with limited resources to participate. However, in its original formulation, SWARM requires frequent gradient synchronization within pipeline stages. 

This work proposes \textit{DiLoCo-SWARM}, a novel approach that integrates DiLoCo-style infrequent gradient synchronization into SWARM parallelism. Our experimental results are summarized in the following contributions:

\begin{enumerate}
  \item \textbf{Convergence.} We demonstrate that SWARM parallelism can effectively integrate DiLoCo and achieve comparable validation perplexity to a synchronous SWARM baseline, while requiring significantly 50x fewer gradient updates.
  \item \textbf{Communication Efficiency.} We show that DiLoCo-SWARM achieves increasing communication savings over SWARM as the model size grows. In smaller models, pipeline communication dominates and limits the impact of our method. However, our approach significantly reduces overall communication costs in larger models, where data parallel communication dominates
  \item \textbf{Robustness.} We validate the robustness of DiLoCo-SWARM to synchronization frequency, and model sizes, underlining its robustness at scale.
\end{enumerate}

We open-source the experiment code and results on \github{} and \wandb{}, along with a distilled version of the training logic in a \gist{} with only a few hundred lines of PyTorch code. We aim for these resources to foster further open-source research on scaling DiLoCo-SWARM parallelism to production-grade systems, following the trajectory of DiLoCo's development.

\section{Background}

This section outlines the foundations of distributed and decentralized learning, detailing core parallelization techniques, key challenges, and recent solutions, with a focus on the DiLoCo and SWARM methods that underpin our proposed approach.

\subsection{Distributed Training}

In distributed training, $n$ nodes collaboratively train a \textit{model} $f_{\theta}$ parameterized by weights $\theta$, on a \textit{dataset} of samples $D = \{(\mathbf{x}_1, \mathbf{y}_1),\dots\}$. This collaboration requires periodic communication of intermediate results between nodes. We will describe two fundamental parallelization techniques relevant to DiLoCo-SWARM.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \vspace{0.5cm}
        \includegraphics[width=\textwidth]{figures/dp.pdf}
        \caption{\textbf{DP Communication}. Nodes synchronize gradients (\bluecircle) at each step within a DP group.}
        \label{fig:dp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \vspace{0.5cm}
        \includegraphics[width=\textwidth]{figures/pp.pdf}
        \caption{\textbf{PP Communication}. Nodes communicate activations and gradients (\orangebox) between adjacent stages within a PP group.}
        \label{fig:pp}
    \end{subfigure}
    \caption{\textbf{DP and PP.} Illustration of communication patterns for $n=2$ DP and PP groups over $T$ training steps. Nodes are represented as boxes, and communication as solid lines. The dotted lines indicate no communication.}
\end{figure}

In \textit{Data Parallelism} (DP)~\cite{dean2012dp}, the dataset is partitioned into $n$ shards, with each node holding a unique shard $D_i$ and a complete model replica $\theta$. Training proceeds as follows: Each node samples a batch from its local data shard, and computes local gradients through a forward and backward pass. Before updating the model, nodes synchronize gradients to ensure identical updates across all nodes (Algorithm~\ref{alg:dp}). Typically, the synchronization is performed using an \texttt{AllReduce} operation. Figure~\ref{fig:dp} illustrates this communication pattern unrolled over $T$ training steps.

\begin{algorithm}
\caption{Data Parallel Gradient Synchronization}
\label{alg:dp}
\begin{algorithmic}[1]
\State {\bfseries Input:} Local Dataset $D_i$, Model $\theta^{(t-1)}$, Optimizer $\mathtt{OPT}$, Loss $\mathcal{L}$
\State Sample batch: $x_i\sim D_i$
\State Compute gradient: $g_i \gets \nabla_{\theta^{(t-1)}} \mathcal{L}(x_i; \theta^{(t-1)})$
\State Sync gradients: $g \gets \frac{1}{n}\sum_{i}^n g_i$ \Comment{$\mathtt{AllReduce}$}
\State Update model: $\theta^{(t)} \gets \mathtt{OPT}(\theta^{(t-1)}, g)$
\end{algorithmic}
\end{algorithm}

In \textit{Pipeline Parallelism} (PP)~\cite{huang2019gpipe}, the model is partitioned into $n$ stages, with each stage holding a unique subset of parameters $\theta_i$. The full model is then represented as a pipeline, where each stage processes a portion of the input sequentially. Communication occurs in two phases during training: activations are sent forward through the pipeline during the forward pass, and gradients are sent backward during the backward pass (Figure~\ref{fig:pp}). Importantly, PP relies solely on point-to-point communication between adjacent stages without all-to-all synchronization.

Despite its simplicity, PP is challenging to optimize in practice due to synchronization delays that cause GPU idle time. For example, the first stage must wait for the full forward and backward pass through the entire pipeline before it can process the next input. Efficient PP implementations, therefore, rely on techniques such as micro-batching and advanced scheduling to minimize idle time and maximize throughput~\cite{harlap2018pipedream, huang2019gpipe}. Further, for large models, the computation-to-communication ratio grows in favor of PP communication, allowing to "hide" communication latency behind computation~\cite{fernandez2024scalingtrends}.

Both parallelization techniques may be combined. We will refer to this form of hybrid parallelism as \textit{Data-Pipeline Parallelism (DPP)}. In this approach, both the dataset and model are partitioned into $\{D_i\}_{i \in [m]}$ and $\{\theta_j\}_{j \in [n]}$, resulting in a two-dimensional grid of $m \times n$ nodes, where the $(i,j)$-th node is responsible for data shard $D_i$ and model shard $\theta_j$. Figure~\ref{fig:dpp} illustrates the communication pattern for a $2\times 2$ DPP system. PP and DP communication is interleaved: First, nodes within each PP group communicate activations and gradients to accumulate local gradients independently. Then, nodes synchronize local gradients within each DP group before updating their model shard.

\begin{figure}[ht]
    \centering
    \vspace{0.5cm}
    \includegraphics[width=0.48\textwidth]{figures/dpp.pdf}
    \caption{\textbf{DPP.} Illustration of communication patterns in a hybrid parallelism (DPP) setup with a $2 \times 2$ grid of nodes. PP and DP communication are interleaved: PP groups accumulate local gradients independently. Then, DP groups synchronize gradients before updating the model.}
    \label{fig:dpp}
\end{figure}

All described parallelization techniques produce equivalent gradient updates and converge to identical model parameters $\theta^{(t)}$. However, they differ fundamentally in how computation and communication are distributed, which impacts efficiency depending on model size, hardware, and network configuration~\cite{hagemann2024parallelization, fernandez2024scalingtrends}.

\subsection{Decentralized Learning}

Decentralized learning builds on foundational work in federated learning (FL)~\cite{mcmahan2016fl}, which enables learning on a network
of mobile devices that retain private access to their own data and periodically share local model updates with a central server for aggregation~\cite{mcmahan2016fl,wang2020fedma}. More recently, decentralized learning has shifted toward large-scale training on preemptible, low-cost hardware, often referred to as \textit{volunteer computing}. While many FL concepts remain relevant, modern research in decentralized learning focuses on improving training efficiency under the unique constraints of decentralized environments: orders-of-magnitude slower interconnect, network and device heterogeneity, and instances joining and leaving training at any time.

Recent research addresses all of these challenges: \citeauthor{zhang2020volatileinstances} first explored fault-tolerant training on pre-emptible instances, which SWARM extended to support hybrid parallelism in heterogeneous environments~\cite{ryabinin2023swarm}. Another key area of research concerns reducing the communication volume of decentralized training to combat the slow interconnect. DiLoCo~\cite{douillard2023diloco} reduces the gradient synchronization frequency via a dual optimization scheme, while DeMo~\cite{peng2024demo} synchronizes only fast-moving momentum components. SPARTA~\cite{exo2025sparta} proposes to only communicate a small random subset of gradients at each step.

Efforts to scale decentralized training to production systems are ongoing. Notable examples include collaborative training platforms such as Transformers Together~\cite{diskin2021collaborativelearning} and Petals~\cite{borzunov2023petals}, which enable distributed inference for large language models. The recent INTELLECT-1 run~\cite{jaghouar2024intellect1} demonstrated the feasibility of decentralized training at scale by collaboratively training a 10-billion-parameter model across the globe.

\subsection{DiLoCo}

In decentralized training, synchronizing gradients at every step, as in traditional DP (Algorithm~\ref{alg:dp}), can be prohibitive due to slow interconnects. DiLoCo~\cite{douillard2023diloco} addresses this limitation by reducing synchronization frequency through a local-global optimization scheme, shown in Algorithm~\ref{alg:diloco}. Each node trains locally for a fixed number of steps, $H$, using a local optimizer. After $H$ steps, nodes compute their pseudo-gradient (the difference between the initial and updated parameters) and synchronize it using a global optimizer to update a shared model.

\begin{algorithm}
\caption{DiLoCo Gradient Synchronization}
\label{alg:diloco}
\begin{algorithmic}
\State \textbf{Input:} Local dataset $D_i$, Model $\theta^{(t-1)}$, Optimizers $\mathtt{OPT}_L$, $\mathtt{OPT}_G$, Loss $\mathcal{L}$, Num. Local Steps $H$ 
\State Copy global model: $\theta_i^{(t-1)} \gets \theta^{(t-1)}$
\For{$H$ steps}
  \State Sample batch: $x \sim D_i$
  \State Compute gradient: $g_i \gets \nabla_{\theta_i} \mathcal{L}(x_i; \theta_i^{(t-1)})$
  \State Update local model: $\theta_i^{(t-1)} \gets \mathtt{OPT}_L(\theta_i^{(t-1)}, g_i)$
\EndFor
\State Compute pseudo-gradient: $\Delta_i \gets \theta_i^{(t-1)} - \theta^{(t-1)}$
\State Sync pseudo-gradients: $\Delta \gets \frac{1}{n} \sum_{i=1}^n \Delta_i$ \Comment{$\mathtt{AllReduce}$}
\State Update global model: $\theta^{(t)} \gets \mathtt{OPT}_G(\theta^{(t-1)}, \Delta)$
\end{algorithmic}
\end{algorithm}

Figure~\ref{fig:diloco} illustrates the communication pattern of DiLoCo in contrast to DP (Figure~\ref{fig:dp}). Nodes only synchronize pseudo-gradients every $H$ steps, thus reducing communication costs by $1/H$. Empirical results show that DiLoCo with $H=500$ matches the generalization performance of a synchronous DP baseline while reducing communication by a factor of 500~\cite{douillard2023diloco,jaghouar2024opendiloco}. However, like DP, DiLoCo is limited by the memory capacity of individual nodes, making it unsuitable for models that exceed local memory. Scaling DiLoCo requires techniques such as gradient quantization~\cite{jaghouar2024intellect1} or parameter offloading~\cite{cui2016}.

\begin{figure}[ht]
    \centering
    \vspace{0.5cm}
    \includegraphics[width=0.45\textwidth]{figures/diloco.pdf}
    \caption{\textbf{DiLoCo.} Comparison of communication patterns for a $n=2$ DiLoCo group over $T$ training steps. Unlike traditional DP, DiLoCo synchronizes pseudo-gradients (\purplecircle) every $H=2$ steps, reducing total communication by a factor of $1/2$.}
    \label{fig:diloco}
\end{figure}

\subsection{SWARM}

While DiLoCo reduces communication frequency, it does not address memory limitations. Sharding the model across nodes, as in pipeline parallelism (PP), is a natural solution. However, traditional PP is poorly suited to decentralized environments due to its sequential nature, which causes significant idle time when nodes have heterogeneous performance. Additionally, a single node failure can stall the entire pipeline.

SWARM parallelism~\cite{ryabinin2023swarm} addresses these limitations by introducing a fault-tolerant data-pipeline parallelism (DPP) approach. Rather than relying on a fixed grid of nodes, SWARM constructs stochastic pipelines dynamically. During the forward pass, each node forwards activations to a randomly selected peer in the next stage, with probability proportional to the peer's throughput. The backward pass follows the same path in reverse to ensure gradient consistency. Once all micro-batches are processed, nodes within each stage form DP groups and synchronize local gradients before updating the model.

This \textit{stochastic wiring} approach has two key advantages: (1) it naturally balances workloads in heterogeneous environments by routing activations to faster nodes, and (2) it enables the pipeline to recover from node failures by re-routing activations or gradients to available nodes. As long as each stage has at least one active node, SWARM remains functional.

A key insight of SWARM is the so-called \textit{Square-Cube Law}~\cite{ryabinin2023swarm}, which states that the communication cost of pipeline parallelism grows quadratically while the computation cost grows cubically for increasing model sizes. Hence, nodes spend more time computing than communicating in large models, allowing them to hide the communication latency behind computation. This leads to the unintuitive consequence that larger models become more communication-efficient.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/swarm.pdf}
    \caption{\textbf{SWARM.} Illustration of communication in a $2 \times 2$ SWARM. As in DPP, PP and PP communication is interleaved. Crucially, any node may communicate with any other node in adjacent stages, increasing fault-tolerance and throughput in heterogeneous environments.}
    \label{fig:swarm}
\end{figure}

\section{DiLoCo-SWARM}

SWARM enables large-scale, fault-tolerant data-pipeline parallel (DPP) training in decentralized settings by dynamically constructing stochastic pipelines. However, its data-parallel component still requires frequent gradient synchronization, resulting in high communication costs. In contrast, DiLoCo~\cite{douillard2023diloco} reduces synchronization frequency through local-global optimization but is limited by memory constraints in large models.

This motivates \textit{DiLoCo-SWARM}, a decentralized training method that combines the communication efficiency of DiLoCo with the scalability and fault-tolerance of SWARM. By replacing SWARM's data-parallel groups with DiLoCo groups, DiLoCo-SWARM significantly reduces the frequency for costly
all-to-all gradient synchronization within SWARM stages (Figure~\ref{fig:diloco-swarm}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/diloco-swarm.pdf}
    \caption{\textbf{DiLoCo-SWARM.} The communication patterns of DiLoCo-SWARM for $4$ nodes in a $2\times 2$ grid. DiLoCo-SWARM replaces SWARM's frequent gradient synchronization with less frequent DiLoCo-style synchronization.}
    \label{fig:diloco-swarm}
\end{figure}

\subsection{Algorithm}

Algorithm~\ref{alg:diloco-swarm} describes the algorithmic details of DiLoCo-SWARM. For simplicity, we assume a grid of $m \times n$ nodes, though the method generalizes to any number of nodes per stage. 

Each node begins with a copy of the global model shard $\theta_j^{(t-1)}$. During the inner optimization loop, nodes perform $H$ local steps, stochastically forwarding activations and gradients to adjacent stages and updating their local model shard $\theta_{i,j}$ using a local optimizer. After $H$ steps, nodes compute pseudo-gradients $\Delta_{i,j}$ and synchronize within their stage to update the global model shard $\theta_j^{(t)}$ using a global optimizer.

DiLoCo-SWARM combines SWARM's stochastic wiring during local steps with DiLoCo to synchronize within SWARM stages. In doing so, the method reduces the points
of gradient synchronization, resulting in more communication-efficient training. Unlike DiLoCo, which cuts communication by a factor of $H$, DiLoCo-SWARM cuts communication by a factor of $H$ times the fraction of of total communication cost incurred by gradient synchronization. Our experiment analyze how this affects real-world systems and communication. Furthermore, due to stochastic wiring, gradients may depend on batches sampled from different data shards, resulting in data mixing across nodes.

\begin{algorithm}
\caption{DiLoCo-SWARM}
\label{alg:diloco-swarm}
\begin{algorithmic}[1]
\State \textbf{Input:} Data shard $D_i$, Model shard $f^{(j)}$, Global model shard parameters $\theta_{j}^{(t-1)}$, Loss $\mathcal{L}$, Optimizers $\mathtt{OPT}_{L}$ and $\mathtt{OPT}_{G}$, Local Steps $H$
\Procedure{FwdBwd}{}
  \If{$j = 1$} \Comment{First stage}
    \State Sample batch: $x_i \sim D_i$
    \State Forward: $a_{ij} \gets f^{(j)}_{\theta_{ij}}(x_i)$
    \State Send forward: $\mathtt{send}(a_{ij}, j+1)$ 
    \State Receive backward: $g_{i,j+1} \gets \mathtt{recv}(j+1)$
    \State Compute gradient: $g_{ij} \gets g_{i,j+1} \cdot \nabla_{\theta_{ij}} f^{(j)}_{\theta_{ij}}(a_{ij})$
  \ElsIf{$j = n$} \Comment{Last stage}
    \State Receive forward: $a_{i,j-1} \gets \mathtt{recv}(j-1)$
    \State Forward: $a_{ij} \gets f^{(j)}_{\theta_{ij}}(a_{i,j-1})$
    \State Compute gradient: $g_{ij} \gets \nabla_{\theta_{ij}} \mathcal{L}(a_{ij})$
    \State Send backward: $\mathtt{send}(g_{ij}, j-1)$
  \Else \Comment{Intermediate stages}
    \State Receive forward: $a_{ij-1} \gets \mathtt{recv}(j-1)$
    \State Forward: $a_{ij} \gets f^{(j)}_{\theta_{ij}}(a_{i,j-1})$
    \State Send forward: $\mathtt{send}(a_{ij}, j+1)$
    \State Receive backward: $g_{i,j+1} \gets \mathtt{recv}(j+1)$
    \State Compute gradient: $g_{ij} \gets g_{i,j+1} \cdot \nabla_{\theta_{ij}} f^{(j)}_{\theta_{ij}}(a_{ij})$
    \State Send backward: $\mathtt{send}(g_{ij}, j-1)$
  \EndIf
  \Return $g_{ij}$
\EndProcedure
\State Copy global model: $\theta_{ij}^{(t-1)} \gets \theta_j^{(t-1)}$
\For{$H$ steps}
  \State Compute gradients: $g_{i,j} \gets \mathtt{FwdBwd}()$
  \State Update local model: $\theta_{ij}^{(t-1)} \gets \mathtt{OPT}_{L}(\theta_{ij}^{(t-1)}, g_{i,j})$
\EndFor
\State Compute pseudo-gradient: $\Delta_{i,j} \gets \theta_{ij}^{(t-1)} - \theta_j^{(t-1)}$
\State Sync pseudo-gradients: $\Delta_j \gets \frac{1}{m}\sum_i^m \Delta_{i,j}$
\State Update global model: $\theta_j^{(t)} \gets \mathtt{OPT}_{G}(\theta_j^{(t-1)}, \Delta_j)$
\end{algorithmic}
\begin{minipage}{\linewidth}
\vspace{1.5ex}
\small
\textbf{Note:} The \texttt{recv} and \texttt{send} functions perform many-to-one and one-to-many communication with adjacent stages using
stochastic wiring. Details are abstracted away for brevity.
\end{minipage}
\end{algorithm}

\subsection{Implementation}

We release a complete implementation of the proposed method, along with a distilled, single-file training script that supports DP, PP, DPP, SWARM, and DiLoCo-SWARM. The implementation is designed for simplicity, relying solely on PyTorch's \texttt{torch.distributed} package for communication. 

The training script is minimal, consisting of only a few hundred lines of code, making it an accessible resource for researchers to explore and experiment with various distributed training techniques. It supports emulation of multiple nodes via threading and integrates with HuggingFace for model and dataset loading. 

While the implementation prioritizes readability over efficiency, it provides a foundation for future work on decentralized training systems. Inspired by open-source efforts like NanoGPT~\cite{karpathy2024nanogpt}, we aim to foster further research and experimentation in decentralized learning.

\section{Experiments}

% Introduction
In this section we report the experiment setup and results validating
DiLoCo-SWARM. The setup and hyperparameters largely follow the
DiLoCo~\cite{douillard2023diloco} experiments.

% Dataset
We consider a language modeling task on the FineWeb-Edu~\cite{penedo2024fineweb}
dataset, a large pre-training dataset consisting of high-quality web pages
filtered for educational content. We choose this dataset over other common
pre-training datasets due to its high token efficiency allowing to match the
original GPT-2 performance on the HellaSwag~\cite{zellers2019hellaswag}
benchmark when training the same model with 10x less
data~\cite{karpathy2024nanogpt}.

% Model
For all experiments, we use the GPT-2 family of models~\cite{radford2019gpt2} in
varying sizes. We adapt the original architectural hyperparameters to match the
original GPT-2 paper~\cite{radford2019gpt2}, as shown in Table~\ref{tab:models}.
Notice that the parameter counts slightly deviate from the original paper, since
we do not share the weights of the embedding matrix and language modeling head
to circumvent additional gradient communication in pipelined settings between
the first and last stage. Per default, we train GPT-2 Small and train from
scratch.

% Model sizes
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Layers} & \textbf{Heads} & \textbf{Hidden Size} & \textbf{Params} \\
\midrule
Tiny & 4 & 4 & 128 & $\sim$14M \\
Small & 12 & 12 & 768 & $\sim$180M \\
Medium & 24 & 16 & 1024 & $\sim$405M \\
Large & 36 & 20 & 1280 & $\sim$800M \\
\bottomrule
\end{tabular}
\caption{\textbf{Model Sizes.} GPT-2 model configurations used in experiments.
Parameter counts account for non-shared weights of embedding matrix and language
modeling head.}
\label{tab:models}
\end{table}

% Hyper-parameters
The hyper-parameters are adapted from the tuning done in
DiLoCo~\cite{douillard2023diloco}: The default local optimizer is
AdamW~\cite{loshchilov2019adamw} with a linearly warmed up learning rate of
$4\cdot 10^{-4}$, and a weight decay of $0.01$. When DiLoCo is active, we use a
Nesterov with a learning rate of $0.7$ and a momentum of $0.9$ as the global
optimizer. A detailed description of the hyperparameters is provided in
Table~\ref{tab:hyperparameters} in the Appendix.

% Evaluation
Our main evaluation metric is the perplexity on a held-out validation set of 10M
randomly sampled tokens from FineWeb-Edu. We evaluate during and after training
to show the convergence against the number of training steps, as well as the
total communication cost.

% Main Experiment: DiLoCo-SWARM
\textbf{Main Experiment.} Our main experiment setup closely follows that of
DiLoCo~\cite{douillard2023diloco}. We train two baselines and a DiLoCo-SWARM.
The weak baseline trains for 2,000 steps on one GPU with a batch size of 512 and
sequence length of 1024, for a total data budget of 1B tokens. The strong
baseline is a 4x2 SWARM, i.e. eight workers distributed evenly into two pipeline
stages. This baseline performs regular gradient synchronization at every step.
The higher compute budget is used to scale the batch size to the number of nodes
per stage, leading to a total training budget of 4B tokens. Finally, we train a
4x2 DiLoCo-SWARM which is identical to the strong baseline but performs outer
optimization steps every 50 steps.

\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiment1-1.pdf}
    \caption{Training Steps}
    \label{fig:experiment1-1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiment1-2.pdf}
    \caption{Communication Cost}
    \label{fig:experiment1-2}
  \end{subfigure}
  \caption{\textbf{Main Result} We show the validation perplexity against the
  number of training steps (left) and the total communication cost (right) for
  two baselines and SWARM-DiLoCo. With the same compute and data budget but with
  50x less gradient synchronization, DiLoCo-SWARM matches the generalization
  performance of the strong SWARM baseline. However, the total communication
  cost is only insignificantly reduced, as the cost is dominated by PP
  communication for GPT-2 Small.}
  \label{fig:experiment1}
\end{figure*}

Figure~\ref{fig:experiment1-1} shows the validation perplexity as a function of
the training steps. Unsurprisingly, the weaker baseline, using less compute and
data, is outperformed by the stronger SWARM baseline with final perplexities of 
37.16 and 30.22, respectively. Our main finding is that DiLoCo-SWARM closely
matches, and even exceeds, the strong baseline in generalization performance
with a validation perplexity of 30.15, despite synchronizing gradients 50x fewer
times. This implies that DiLoCo-style gradient synchronization is compatible
with SWARM parallelism, allowing to reduce the communication cost incurred by 
gradient synchronization within pipeline stages of SWARM.

Figure~\ref{fig:experiment1-2} shows the total communication cost for the same
experiments. Surprisingly, the total communication cost is only moderately
reduced in DiLoCo-SWARM compared to SWARM. This finding points at a an important
difference between DiLoCo and DiLoCo-SWARM. In DiLoCo, synchronizing gradients
every $H$ translates to a directly proportional reduction in communication cost
by $1/H$. DiLoCo-SWARM, in contrast, interleaves DP and PP communication. Since
DiLoCo can only reduce DP communication cost, the PP communication stays
constant.  Hence, we can only hope to reduce the total communication cost as a
factor of $1/H \cdot \text{DP Communication Cost Fraction}$. Luckily, the
square-cube law~\cite{ryabinin2023swarm} suggests that the PP communication cost
scales quadratically while DP communication scales cubically with increasing
model sizes. Thus, with larger models, DP communication dominates the total
communication cost. We illustrate this phenomenon in
Figure~\ref{fig:square-cube-law}, which shows the fraction of the communication
cost incurred by DP and PP communication for 4x2 SWARMs training increasingily
large models. In summary, DP communication dominates the total communication
cost for larger models, and so DiLoCo-SWARM is only able to reduce the
communication cost by $1/H$ in the limit of large models.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/square-cube-law.pdf}
  \caption{\textbf{Communication Cost Scaling} We show the (log) communication
  cost incurred by DP and PP communication for 4x2 SWARMs in the common training
  setup. Because of the square-cube law, PP communication dominates the total communication for smaller models, but DP communication dominates for larger models.} 
  \label{fig:square-cube-law}
\end{figure}

% Experiment 2: Ablation on communication frequency
\textbf{Communication Frequency.} We are interested in how \textit{infrequently}
gradient synchronization can occur within SWARM pipeline stages without
impacting convergence. Intuitively, the slower the frequency, the worse
convergence, but whether synchronizing every 10 or 200 steps negatively impacts
performance, has implications on the practical usefulness of the algorithm. To
investigate, we train five models using the same training setup and only vary
the number of local training steps before performing synchronizing gradients.

% TODO: Add perplexities in legend
% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=0.45\textwidth]{figures/experiment3.png}
%   \caption{\textbf{Communication Frequency} We vary the number of local training
%   steps before performing an outer optimization step from $\{10, 20, 50, 100, 200\}$ 
%   and report the validation perplexity throughout training. Synchronizing more
%   frequently yields better results but performance degradation is neglibible even
%   on all tested values.}
%   \label{fig:experiment2}
% \end{figure}

Table~\ref{tab:experiment2} shows the results of this experiment. In line with
the original DiLoCo results, generalization performance monotonically increases
with higher gradient synchronization frequency. However, the performance
degradation for less frequent gradient synchronization is mild. Synchronizing
every 10 steps achieves the best performance, with a validation perplexity of
27.95. Note, that this also significantly out-performs the SWARM baseline in
Figure~\ref{fig:experiment1} while still reducing the synchronization frequency
by 10x. In our experiments, a good trade-off between communication frequency and
performance is achieved when synchronizing every 50 steps, which is therefore
used throughout all other experiments.

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Freq.} & \textbf{Val. PPL} & \textbf{$\Delta$ (Abs./Rel.)} \\ 
\midrule
10 & 27.95 & - \\
20 & 28.61 & 0.66 / 2.36\% \\
50 & 30.15 & 2.2 / 7.87\% \\
100 & 30.49 & 2.54 / 9.09\% \\
200 & 31.27 & 3.32 / 11.88\% \\
\bottomrule
\end{tabular}
\caption{\textbf{Communication Frequency} We report the final validation
perplexity for different communication frequencies and their absolute and
relative changes compared to synchronizing every 10 steps.}
\label{tab:experiment2}
\end{table}

% Experiment 3: Varying the model size
\textbf{Model Sizes.} Finally, we train three sizes of GPT-2 models, as 
described in Table~\ref{tab:models} to assess whether DiLoCo-style gradient
synchronization is robust scaling the model. We use the same hyper-parameters
for all models from Table~\ref{tab:hyperparameters} and train for 2,000 steps.
For each size, we also train a single-node baseline with four times smaller
batch size.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/experiment3.pdf}
  \caption{\textbf{Model Size} We train three different sizes of GPT-2 models and report the validation perplexity throughout training. The performance improvement over the single-node baseline remains constant at around 18\% across all model sizes, suggesting that DiLoCo-style gradient synchronization scales well with increasing model size.}
  \label{fig:experiment3}
\end{figure}

% \begin{table}[ht]
% \centering
% \begin{tabular}{lccc}
% \toprule
% \textbf{\# Params} & \textbf{PPL} & \textbf{$\Delta$ (Abs./Rel.)} \\ 
% \midrule
% 180M & 30.15 & 7.01 / 18.86\% \\
% 400M & 25.66 & 5.91 / 18.72\% \\
% 800M & & & \\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Model Size} We report the final validation perplexity for
% different model sizes and their absolute and relative changes compared to a
% single-node baseline with smaller batch size.}
% \label{tab:experiment3}
% \end{table}

Figure~\ref{fig:experiment3} shows with DiLoCo we obtain a constant improvement
of $\sim$18\% over the single-node baseline run, for all tested model sizes.
This finding suggests that gradient synchronization with DiLoCo scales to larger
models sizes. Especially in the light of the square-cube law, the scaling
property of DiLoCo is crucial to be viable to train large models using
DiLoCo-SWARM.

\section{Conclusion}

The results of this study suggest that DiLoCo-style gradient synchronization is
compatible with SWARM parallelism. This implies that gradient 
synchronization via a dual optimization scheme is an effective tool for gradient
synchronization, even when the gradients correspond to model shards in pipeline
stages accumulated via stochastic forward and backward passes.

The findings have implications both for the SWARM and DiLoCo community. For
SWARM, this means that the communication cost incurred by gradient
synchronization within pipeline stages can be drastically reduced. Hence, future
research and open-source efforts should focus on integrating DiLoCo-style 
gradient synchronization into production implementations of SWARM. For DiLoCo,
the findings further underline the robustness of the method, suggesting that
DiLoCo is a good candidate as a drop-in replacement anytime gradients need to be
synchronized between nodes in distributed settings.

\section{Future Work}

% No tuning
This work assumed that hyperparameters found in
DiLoCo~\cite{douillard2023diloco} apply directly to DiLoCo-SWARM. While it is
likely that the hyperparameter provide a good starting point, it is perceivable
that a slightly different set of hyperparameters might yield better results in
the training setup of this work due to differences in the dataset, model and
training algorithm. An immediate continuation of this work would therefore be to
conduct a thorough hyper-parameter sweeps DiLoCo-SWARM and the considered
baselines, focusing on ones that are most relevant to convergence properties of 
the training, i.e. batch size, inner and outer learning rate and learning rate
schedule. It is to be noted that finding the right trade-off for the compute
budget is not always easy: Ideally, we would like to train for as long as
possible to obtain the most robust set of hyperparameters, but this is feasible
when models when training on huge scale. Hence, hyper-parameter tuning has to be
conducted on smaller scale and then be extrapolated to larger scales.

% Scale
By today's standards, the GPT-2 family of models is small. Still, training on
less than 10B tokens means that models are still in the early stage of training.
The perplexities are still decreasing, and so the results are not yet
representative of the full training performance. For example, in the original
DiLoCo and SWARM experiments were conducted on a data budget of roughly 50B
tokens per run, resulting in multiple GPU days worth of compute. While the
results are still indicative of the efficacy of the algorithm, future work
should simply let the models train for longer to obtain more robust results.

% Environment
Finally, all experiments were conducted on co-located GPUs with reliable and fast
interconnectivity. The initial premise of this research, however, was the
decentralized setting with unreliable, heterogeneous and poorly connected
compute resources. SWARM is only favourable over traditional parallelization
techniques in this setting, and so while the experiments in this study suggest
that DiLoCo-style gradient synchronization can be used in SWARM, it remains to
show how the method behaves in a truly decentral setting when scaled to larger
models, requiring more pipeline stages and nodes per stage.

% Efficiency
All of the above point to more more scale and compute. A crucial step is to
integrate optimized implementations of DiLoCo and SWARM, including efficient
pipeline scheduling techniques, allowing for fault-tolerant training over-the-
internet. Only then experiments can be conducted on truly larger scale and in a
realistic environment.

% SWARM architecture
Finally, another interesting, but open, research question is how the
DiLoCo-SWARM behaves when varying the number of workers per stage, which is not
discussed in this work but has implications for the scalability of the method.

\section*{Acknowledgements}
\label{sec:acknowledgements}

This work was developed in collaboration with the Scalable Computing Systems
(SaCS) lab at EPFL as well as Prime Intellect. All compute was kindly sponsored
by Prime Intellect.

% Bibliography
\bibliography{references}
\bibliographystyle{icml2023}

% Appendix
\newpage
\appendix
\onecolumn

\section{Appendix}

% Hardware
\subsection{Hardware}

All experiments were conducted on a single node with eight co-located H100 GPUs
on \href{https://app.primeintellect.com/}{Prime Intellect Compute}.

\subsection{Training Script Invocations}

Below we show example invocations for different distributed training algorithms.
All examples use a GPT-2 small model and the FineWeb-Edu dataset as an example.
For more details see the README on \github.

\begin{lstlisting}[language=bash]
# Single GPU training
torchrun --nproc_per_node 1 src/train.py --swarm.num_stages 1 \
    --model @configs/model/gpt2-small.toml \
    --data @configs/data/fineweb-edu-10bt.toml
\end{lstlisting}

\begin{lstlisting}[language=bash]
# Pipeline parallel training with 2 GPUs
torchrun --nproc_per_node 2 src/train.py --swarm.num_stages 2 \
    --model @configs/model/gpt2-small.toml \
    --data @configs/data/fineweb-edu-10bt.toml
\end{lstlisting}

\begin{lstlisting}[language=bash]
# Data parallel training with 2 GPUs
torchrun --nproc_per_node 2 src/train.py --swarm.num_stages 2 \
    --model @configs/model/gpt2-small.toml \
    --data @configs/data/fineweb-edu-10bt.toml
\end{lstlisting}

\begin{lstlisting}[language=bash]
# DiLoCo training with 2 GPUs
torchrun --nproc_per_node 2 src/train.py --swarm.num_stages 2 \
    --train.outer_optimizer @configs/optimizer/nesterov.toml
    --model @configs/model/gpt2-small.toml \
    --data @configs/data/fineweb-edu-10bt.toml
\end{lstlisting}

\begin{lstlisting}[language=bash]
# SWARM training with 4 GPUs
torchrun --nproc_per_node 4 src/train.py --swarm.num_stages 2 \
    --model @configs/model/gpt2-small.toml \
    --data @configs/data/fineweb-edu-10bt.toml
\end{lstlisting}

\begin{lstlisting}[language=bash]
# DiLoCo-SWARM training with 4 GPUs
torchrun --nproc_per_node 4 src/train.py --swarm.num_stages 2 \
    --train.outer_optimizer configs/optimizer/nesterov.toml
    --model @configs/model/gpt2-small.toml \
    --data @configs/data/fineweb-edu-10bt.toml
\end{lstlisting}

\subsection{Training Script Feature List}

Table~\ref{tab:features} shows some of the key features supported by the
training script, and features that are not yet supported. The fault tolerance
mechanisms in SWARM, re-wiring and adaptive rebalancing, are difficult to
implement in \texttt{torch.dist} as it does not support dynamic world sizes.
Note that Single-Node training and DP, PP, DPP training are all feature subsets
of SWARM stochastic wiring and so the script can be used to train using these
algorithms simply by setting the appropriate parameters (see above).

\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Feature} & \textbf{Supported?} \\ 
\midrule
DP Gradient Synchronization & \checkmark \\
DiLoCo Gradient Synchronization & \checkmark \\
SWARM Stochastic Wiring & \checkmark \\
SWARM Rewiring & \\
SWARM Adaptive Rebalancing & \\
\bottomrule
\end{tabular}
\caption{Training Script Feature List}
\label{tab:features}
\end{table}

% Hyperparameters
\subsection{Hyperparameters}

Table~\ref{tab:hyperparameters} shows the hyperparameters used throughout the
experiments. The outer optimizer parameters is only used for DiLoCo-style
training. For non-DiLoCo runs gradients are averaged before performing a regular
local optimizer step.

\begin{table}[ht]
\centering
\begin{tabular}{llc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\ 
\midrule
\multirow{1}{*}{General} & Batch Size & 512 \\ 
& Sequence Length & 1024 \\ 
& Steps & 2000 \\
\hline
\multirow{1}{*}{Local Optimizer} & Name & AdamW \\ 
& Weight decay & - \\ 
& Learning Rate & $4 \times 10^{-4}$ \\ 
\hline
\multirow{1}{*}{Global Optimizer} & Name & Nesterov \\ 
& Learning Rate & 0.7 \\ 
& Momentum & 0.9 \\ 
\bottomrule
\end{tabular}
\caption{Hyperparameters}
\label{tab:hyperparameters}
\end{table}


\end{document}