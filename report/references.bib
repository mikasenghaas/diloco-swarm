@misc{mcmahan2016fl,
      title={{Communication-Efficient Learning of Deep Networks from Decentralized Data}}, 
      author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
      year={2016},
      eprint={1602.05629},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.05629}, 
}

@misc{wang2020fedma,
      title={{Federated Learning with Matched Averaging}}, 
      author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
      year={2020},
      eprint={2002.06440},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.06440}, 
}

@inproceedings{krizhevsky2012alexnet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}


@misc{vaswani2017transformer,
  title = {{Attention Is All You Need}},
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit
            and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia
            Polosukhin},
  year = {2023},
  eprint = {1706.03762},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1706.03762},
}

@misc{chowdhery2022palm,
      title={{PaLM: Scaling Language Modeling with Pathways}},
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.02311}, 
}

@misc{radford2019gpt2,
  author = {{Radford, A. and Metz, L. and Chintala, S.}},
  title = {{Language Models are Unsupervised Multitask Learners}},
  year = {2019},
  howpublished = {\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}},

}  

@misc{brown2023gpt3,
  title = {{Language Models are Few-Shot Learners}},
  author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah
            and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and
            Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal
            and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and
            Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu
            and Clemens Winter and Christopher Hesse and Mark Chen and Eric
            Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack
            Clark and Christopher Berner and Sam McCandlish and Alec Radford and
            Ilya Sutskever and Dario Amodei},
  year = {2020},
  eprint = {2005.14165},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2005.14165},
}

@misc{dubey2024llama3,
  title = {{The Llama 3 Herd of Models}},
  author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and \dots},
  year = {2024},
  eprint = {2407.21783},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2407.21783},
}

@misc{google2024gemini,
      title={{Gemini: A Family of Highly Capable Multimodal Models}}, 
      author={Gemini},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{shoeybi2020megatron,
  title = {{Megatron-LM: Training Multi-Billion Parameter Language Models Using
           Model Parallelism}},
  author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick
            LeGresley and Jared Casper and Bryan Catanzaro},
  year = {2020},
  eprint = {1909.08053},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1909.08053},
}

@misc{weisser2024pi,
  title = {{Introducing Prime Intellect}},
  howpublished = {\url{
                  https://www.primeintellect.ai/blog/introducing-prime-intellect}
                  },
  note = {Accessed: 2024-09-24},
}

@misc{loshchilov2019adamw,
  title = {{Decoupled Weight Decay Regularization}},
  author = {Ilya Loshchilov and Frank Hutter},
  year = {2019},
  eprint = {1711.05101},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/1711.05101},
}

@misc{yuan2022,
  title = {{Decentralized Training of Foundation Models in Heterogeneous
           Environments}},
  author = {Binhang Yuan and Yongjun He and Jared Quincy Davis and Tianyi Zhang
            and Tri Dao and Beidi Chen and Percy Liang and Christopher Re and Ce
            Zhang},
  year = {2023},
  eprint = {2206.01288},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  url = {https://arxiv.org/abs/2206.01288},
}

@misc{douillard2023diloco,
  title = {{DiLoCo: Distributed Low-Communication Training of Language Models}},
  author = {Arthur Douillard and Qixuan Feng and Andrei A. Rusu and Rachita
            Chhaparia and Yani Donchev and Adhiguna Kuncoro and Marc'Aurelio
            Ranzato and Arthur Szlam and Jiajun Shen},
  year = {2023},
  eprint = {2311.08105},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2311.08105},
}

@misc{jaghouar2024opendiloco,
  title = {{OpenDiLoCo: An Open-Source Framework for Globally Distributed
           Low-Communication Training}},
  author = {Sami Jaghouar and Jack Min Ong and Johannes Hagemann},
  year = {2024},
  eprint = {2407.07852},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2407.07852},
}

@misc{jaghouar2024intellect1,
      title={{INTELLECT-1 Technical Report}}, 
      author={Sami Jaghouar and Jack Min Ong and Manveer Basra and Fares Obeid and Jannik Straube and Michael Keiblinger and Elie Bakouch and Lucas Atkins and Maziyar Panahi and Charles Goddard and Max Ryabinin and Johannes Hagemann},
      year={2024},
      eprint={2412.01152},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2412.01152}, 
}

@misc{ryabinin2023swarm,
  title = {{SWARM Parallelism: Training Large Models Can Be Surprisingly
           Communication-Efficient}},
  author = {Max Ryabinin and Tim Dettmers and Michael Diskin and Alexander
            Borzunov},
  year = {2023},
  eprint = {2301.11913},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  url = {https://arxiv.org/abs/2301.11913},
}

@misc{tang2020sparse,
  title = {{Communication-Efficient Decentralized Learning with Sparsification
           and Adaptive Peer Selection}},
  author = {Zhenheng Tang and Shaohuai Shi and Xiaowen Chu},
  year = {2020},
  eprint = {2002.09692},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2002.09692},
}

@inproceedings{cui2016,
author = {Cui, Henggang and Zhang, Hao and Ganger, Gregory R. and Gibbons, Phillip B. and Xing, Eric P.},
title = {GeePS: scalable deep learning on distributed GPUs with a GPU-specialized parameter server},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901323},
doi = {10.1145/2901318.2901323},
abstract = {Large-scale deep learning requires huge computational resources to train a multi-layer neural network. Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections. While the computation involved can be done more efficiently on GPUs than on more traditional CPU cores, training such networks on a single GPU is too slow and training on distributed GPUs can be inefficient, due to data movement overheads, GPU stalls, and limited GPU memory. This paper describes a new parameter server, called GeePS, that supports scalable deep learning across GPUs distributed among multiple machines, overcoming these obstacles. We show that GeePS enables a state-of-the-art single-node GPU implementation to scale well, such as to 13 times the number of training images processed per second on 16 machines (relative to the original optimized single-node code). Moreover, GeePS achieves a higher training throughput with just four GPU machines than that a state-of-the-art CPU-only system achieves with 108 machines.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {4},
numpages = {16},
location = {London, United Kingdom},
series = {EuroSys '16}
}

@misc{penedo2024fineweb,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}, 
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      eprint={2406.17557},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17557}, 
}

@misc{zellers2019hellaswag,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.07830}, 
},

@misc{karpathy2024nanogpt,
  author = {{Karpathy, Andrej}},
  title = {{NanoGPT}},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
  commit = {93a43d9a5c22450bbf06e78da2cb6eeef084b717}
}


@misc{deepseekai2024,
      title={{DeepSeek-V3 Technical Report}}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{moddednanogpt2024,
  title        = {{modded-nanogpt: Speedrunning the NanoGPT baseline}},
  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and
                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and
                  Franz Cesista and Braden Koszarsky and @Grad62304977},
  year         = {2024},
  url          = {https://github.com/KellerJordan/modded-nanogpt},
  note         = {Accessed: 2025-01-03}
}

@article{dean2012dp,
  title = {{Large Scale Distributed Deep Networks}},
  author = {Dean, Jeffrey and Corrado, G.s and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc and Mao, Mark and Ranzato, Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew},
  year = {2012},
  month = {10},
  pages = {},
  journal = {Advances in neural information processing systems}
}
 
@article{walker1995mpi,
title = {{Mpi: A Standard Message Passing Interface}},
author = {Walker, David and Dongarra, Jack},
year = {1995},
month = {12},
pages = {},
volume = {12},
journal = {Supercomputer}
}

@misc{huang2019gpipe,
      title={{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}}, 
      author={Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen},
      year={2019},
      eprint={1811.06965},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1811.06965}, 
}

@misc{harlap2018pipedream,
      title={{PipeDream: Fast and Efficient Pipeline Parallel DNN Training}}, 
      author={Aaron Harlap and Deepak Narayanan and Amar Phanishayee and Vivek Seshadri and Nikhil Devanur and Greg Ganger and Phil Gibbons},
      year={2018},
      eprint={1806.03377},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1806.03377}, 
}

@misc{hagemann2024parallelization,
      title={{Efficient Parallelization Layouts for Large-Scale Distributed Model Training}}, 
      author={Johannes Hagemann and Samuel Weinbach and Konstantin Dobler and Maximilian Schall and Gerard de Melo},
      year={2024},
      eprint={2311.05610},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.05610}, 
}

@misc{fernandez2024scalingtrends,
      title={{Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training}}, 
      author={Jared Fernandez and Luca Wehrstedt and Leonid Shamis and Mostafa Elhoushi and Kalyan Saladi and Yonatan Bisk and Emma Strubell and Jacob Kahn},
      year={2024},
      eprint={2411.13055},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.13055}, 
}

@misc{diskin2021collaborativelearning,
      title={{Collaborative Learning in Open Source}}, 
      author={Michael Diskin and Alexey Bukhtiyarov and Max Ryabinin and Lucile Saulnier and Quentin Lhoest and Anton Sinitsin and Dmitry Popov and Dmitry Pyrkin and Maxim Kashirin and Alexander Borzunov and Albert Villanova del Moral and Denis Mazur and Ilia Kobelev and Yacine Jernite and Thomas Wolf and Gennady Pekhimenko},
      year={2021},
      eprint={2106.10207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.10207}, 
}

@misc{borzunov2022transformerstogether,
      title={{Training Transformers Together}}, 
      author={Alexander Borzunov and Max Ryabinin and Tim Dettmers and Quentin Lhoest and Lucile Saulnier and Michael Diskin and Yacine Jernite and Thomas Wolf},
      year={2022},
      eprint={2207.03481},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.03481}, 
}

@inproceedings{borzunov2023petals,
  title = {{Petals: Collaborative Inference and Fine-tuning of Large Models}},
  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and
            Riabinin, Maksim and Belkada, Younes and Chumachenko, Artem and
            Samygin, Pavel and Raffel, Colin},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 3: System Demonstrations)},
  pages = {558--568},
  year = {2023},
  url = {https://arxiv.org/abs/2209.01188},
}

@misc{peng2024demo,
      title={{DeMo: Decoupled Momentum Optimization}}, 
      author={Bowen Peng and Jeffrey Quesnelle and Diederik P. Kingma},
      year={2024},
      eprint={2411.19870},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.19870}, 
}

@misc{exo2025sparta,
  title = {{SPARTA: Distributed Training with Sparse Parameter Averaging}},
  author = {EXO},
  year = {2025},
  howpublished = {\url{https://blog.exolabs.net/day-12/}},
  note = {Accessed: 2025-01-08},
}

% From SWARM related work
@misc{zhang2020volatileinstances,
      title={Machine Learning on Volatile Instances}, 
      author={Xiaoxi Zhang and Jianyu Wang and Gauri Joshi and Carlee Joe-Wong},
      year={2020},
      eprint={2003.05649},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2003.05649}, 
}
