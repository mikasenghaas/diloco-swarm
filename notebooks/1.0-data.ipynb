{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Data\n",
    "\n",
    "This notebook contains code for the data in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict, load_dataset \n",
    "from src.utils import format_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText 2\n",
    "\n",
    "For now, we will usoe a tiny dataset `Salesforce/wikitext/wikitext-2-raw-v1`. It has a train, validation and test split that consist of 37K, 1.8K and 2.2K examples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText 2\n",
    "wiki = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_wiki, val_wiki, test_wiki = wiki[\"train\"], wiki[\"validation\"], wiki[\"test\"]\n",
    "\n",
    "print(f\"Loaded {len(train_wiki)/1e3:.1f}K training, {len(val_wiki)/1e3:.1f}K validation and {len(test_wiki)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single example just has a `text` field, which contains a single line of text. They are parsed from high quality Wikipedia articles. We can already see that there are loads of empty lines and other artiffacts like headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "for example in train_wiki.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove empty lines, headlines, and trailing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_empty_text(examples: Dict[str, Any]) -> bool:\n",
    "    return examples[\"text\"] != \"\"\n",
    "\n",
    "def non_headline(examples: Dict[str, Any]) -> bool:\n",
    "    return not examples[\"text\"].startswith(\" = \")\n",
    "\n",
    "def strip_headline(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    examples[\"text\"] = examples[\"text\"].lstrip().rstrip()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_processed = train_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "val_wiki_processed = val_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "test_wiki_processed = test_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "\n",
    "print(f\"Processed {len(train_wiki_processed)/1e3:.1f}K training, {len(val_wiki_processed)/1e3:.1f}K validation and {len(test_wiki_processed)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_wiki_processed.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's get some statistics on the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_chars = lambda dataset: sum(len(example['text']) for example in dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# Llama 2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test'],\n",
    "    'Examples': map(format_int, [get_num_examples(train_wiki_processed), get_num_examples(val_wiki_processed), get_num_examples(test_wiki_processed)]),\n",
    "    'Characters': map(format_int, [get_num_chars(train_wiki_processed), get_num_chars(val_wiki_processed), get_num_chars(test_wiki_processed)]),\n",
    "    'Tokens': map(format_int, [get_num_tokens(train_wiki_processed, tokenizer), get_num_tokens(val_wiki_processed, tokenizer), get_num_tokens(test_wiki_processed, tokenizer)])\n",
    "}).set_index('Split')\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's push the processed datasets to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "data = DatasetDict({\n",
    "    'train': train_wiki_processed,\n",
    "    'validation': val_wiki_processed,\n",
    "    'test': test_wiki_processed\n",
    "})\n",
    "\n",
    "repo_name = \"wikitext-2\"\n",
    "data.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinewebEdu\n",
    "\n",
    "The [FinewebEdu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) dataset is a large-scale pre-training dataset developed by the Hugging Face team. The smaller version consists of 1.3T high-quality tokens that have been filtered for quality using Llama 2 70B\n",
    "\n",
    "We are going to use the 10BT version of the dataset:\n",
    "- 9.67M Examples\n",
    "- 10.1B (GPT-2) Tokens\n",
    "- 1.3GB Disk Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinewebEdu (10BT)\n",
    "finewebedu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\")\n",
    "\n",
    "print(f\"Loaded {len(finewebedu)/1e6:.1f}M training examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an example\n",
    "for example in finewebedu.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_chars = lambda dataset: sum(len(example['text']) for example in dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# GPT-2 tokenizer\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama2_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3-70b-hf\")\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'Split': ['Train'],\n",
    "    'Examples': [format_int(get_num_examples(finewebedu))],\n",
    "    'Characters': [format_int(get_num_chars(finewebedu))],\n",
    "    'GPT-2 Tokens': [format_int(get_num_tokens(finewebedu, gpt_tokenizer))],\n",
    "    'Llama-2 Tokens': [format_int(get_num_tokens(finewebedu, llama2_tokenizer))],\n",
    "    'Llama-3 Tokens': [format_int(get_num_tokens(finewebedu, llama3_tokenizer))]\n",
    "}).set_index('Split')\n",
    "\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
