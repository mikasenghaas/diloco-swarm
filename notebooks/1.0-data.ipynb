{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Data\n",
    "\n",
    "This notebook contains code for the data in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from src.utils import format_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorizing Dataset\n",
    "\n",
    "We will create a single sample dummy dataset to test that we can overfit a single sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a dataset with a single example in train, validation and test: I am a large language model and I can memorize this sentence.\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset with a single example\n",
    "sentence = \"I am a large language model and I can memorize this sentence.\"\n",
    "dataset = Dataset.from_dict({\"text\": [sentence]})\n",
    "memorize = DatasetDict({\"train\": dataset, \"validation\": dataset, \"test\": dataset})\n",
    "\n",
    "print(f\"Created a dataset with a single example in train, validation and test: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 2015.52ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 2101.35ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 2211.02ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed to https://huggingface.co/datasets/mikasenghaas/memorize\n"
     ]
    }
   ],
   "source": [
    "# Push to Hugging Face Hub\n",
    "repo_name = \"memorize\"\n",
    "memorize.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText 2\n",
    "\n",
    "For now, we will usoe a tiny dataset `Salesforce/wikitext/wikitext-2-raw-v1`. It has a train, validation and test split that consist of 37K, 1.8K and 2.2K examples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 412445.89 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1126560.51 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 728231.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 36.7K training, 3.8K validation and 4.4K test examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load WikiText 2\n",
    "wiki = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", cache_dir=\"/workspace/huggingface\")\n",
    "train_wiki, val_wiki, test_wiki = wiki[\"train\"], wiki[\"validation\"], wiki[\"test\"]\n",
    "\n",
    "print(f\"Loaded {len(train_wiki)/1e3:.1f}K training, {len(val_wiki)/1e3:.1f}K validation and {len(test_wiki)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single example just has a `text` field, which contains a single line of text. They are parsed from high quality Wikipedia articles. We can already see that there are loads of empty lines and other artiffacts like headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ''}\n",
      "{'text': ' = Valkyria Chronicles III = \\n'}\n",
      "{'text': ''}\n",
      "{'text': ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n'}\n",
      "{'text': \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "for example in train_wiki.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove empty lines, headlines, and trailing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_empty_text(examples: Dict[str, Any]) -> bool:\n",
    "    return examples[\"text\"] != \"\"\n",
    "\n",
    "def non_headline(examples: Dict[str, Any]) -> bool:\n",
    "    return not examples[\"text\"].startswith(\" = \")\n",
    "\n",
    "def strip_headline(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    examples[\"text\"] = examples[\"text\"].lstrip().rstrip()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 36718/36718 [00:00<00:00, 402373.52 examples/s]\n",
      "Filter: 100%|██████████| 23767/23767 [00:00<00:00, 168581.05 examples/s]\n",
      "Map: 100%|██████████| 17556/17556 [00:00<00:00, 19673.33 examples/s]\n",
      "Filter: 100%|██████████| 3760/3760 [00:00<00:00, 317187.91 examples/s]\n",
      "Filter: 100%|██████████| 2461/2461 [00:00<00:00, 147487.14 examples/s]\n",
      "Map: 100%|██████████| 1841/1841 [00:00<00:00, 18811.06 examples/s]\n",
      "Filter: 100%|██████████| 4358/4358 [00:00<00:00, 340120.89 examples/s]\n",
      "Filter: 100%|██████████| 2891/2891 [00:00<00:00, 156855.74 examples/s]\n",
      "Map: 100%|██████████| 2183/2183 [00:00<00:00, 18823.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17.6K training, 1.8K validation and 2.2K test examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_wiki_processed = train_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "val_wiki_processed = val_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "test_wiki_processed = test_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "\n",
    "print(f\"Processed {len(train_wiki_processed)/1e3:.1f}K training, {len(val_wiki_processed)/1e3:.1f}K validation and {len(test_wiki_processed)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .'}\n",
      "{'text': \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\"}\n",
      "{'text': \"It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 .\"}\n",
      "{'text': \"As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role .\"}\n",
      "{'text': 'The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon .'}\n"
     ]
    }
   ],
   "source": [
    "for example in train_wiki_processed.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's get some statistics on the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m gpt2_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m llama3_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m stats \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSplit\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExamples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmap\u001b[39m(format_int, [get_num_examples(train_wiki_processed), get_num_examples(val_wiki_processed), get_num_examples(test_wiki_processed)]),\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPT-2 Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmap\u001b[39m(format_int, [get_num_tokens(train_wiki_processed, gpt2_tokenizer), get_num_tokens(val_wiki_processed, gpt2_tokenizer), get_num_tokens(test_wiki_processed, gpt2_tokenizer)]),\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLlama-3 Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmap\u001b[39m(format_int, [\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_wiki_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama3_tokenizer\u001b[49m\u001b[43m)\u001b[49m, get_num_tokens(val_wiki_processed, llama3_tokenizer), get_num_tokens(test_wiki_processed, llama3_tokenizer)])\n\u001b[1;32m     14\u001b[0m })\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSplit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m stats\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(dataset, tokenizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dataset statistics\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m dataset: \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[0;32m----> 3\u001b[0m get_num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m dataset, tokenizer: \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Llama 2 tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m gpt2_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dataset statistics\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m dataset: \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[0;32m----> 3\u001b[0m get_num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m dataset, tokenizer: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m dataset)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Llama 2 tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m gpt2_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/alloc/miniconda/envs/swarm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2627\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2590\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2611\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2614\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2627\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/alloc/miniconda/envs/swarm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3037\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3038\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3039\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3043\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3044\u001b[0m )\n\u001b[0;32m-> 3046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/alloc/miniconda/envs/swarm/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:600\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    578\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    598\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    599\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 600\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/alloc/miniconda/envs/swarm/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:526\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 526\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    538\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    540\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    550\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# Llama 2 tokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test'],\n",
    "    'Examples': map(format_int, [get_num_examples(train_wiki_processed), get_num_examples(val_wiki_processed), get_num_examples(test_wiki_processed)]),\n",
    "    'GPT-2 Tokens': map(format_int, [get_num_tokens(train_wiki_processed, gpt2_tokenizer), get_num_tokens(val_wiki_processed, gpt2_tokenizer), get_num_tokens(test_wiki_processed, gpt2_tokenizer)]),\n",
    "    'Llama-3 Tokens': map(format_int, [get_num_tokens(train_wiki_processed, llama3_tokenizer), get_num_tokens(val_wiki_processed, llama3_tokenizer), get_num_tokens(test_wiki_processed, llama3_tokenizer)])\n",
    "}).set_index('Split')\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's push the processed datasets to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "data = DatasetDict({\n",
    "    'train': train_wiki_processed,\n",
    "    'validation': val_wiki_processed,\n",
    "    'test': test_wiki_processed\n",
    "})\n",
    "\n",
    "repo_name = \"wikitext-2\"\n",
    "data.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinewebEdu\n",
    "\n",
    "The [FinewebEdu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) dataset is a large-scale pre-training dataset developed by the Hugging Face team. The smaller version consists of 1.3T high-quality tokens that have been filtered for quality using Llama 2 70B\n",
    "\n",
    "We are going to use the 10BT version of the dataset which contains 9.67M samples, corresponding to roughly 10B GPT-2 training tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinewebEdu (10BT)\n",
    "finewebedu_10bt = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\", cache_dir=\"/workspace/huggingface\")\n",
    "\n",
    "print(f\"Loaded {len(finewebedu_10bt)/1e6:.1f}M training examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10% of FinewebEdu 10BT (1BT)\n",
    "finewebedu_1bt = finewebedu_10bt.shuffle(seed=42).select(range(int(len(finewebedu_10bt) * 0.1)))\n",
    "\n",
    "print(f\"Sampled {len(finewebedu_1bt)/1e6:.1f}M training examples (10% of 10BT).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10% of FinewebEdu 1.25BT\n",
    "finewebedu_100mt = finewebedu_10bt.shuffle(seed=42).select(range(int(len(finewebedu_1bt) * 0.1)))\n",
    "\n",
    "print(f\"Sampled {len(finewebedu_100mt)/1e6:.1f}M training examples (10% of 1BT).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# GPT-2 tokenizer\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Function to calculate average tokens per example\n",
    "def calc_avg_tokens(dataset, tokenizer, num_samples=10000):\n",
    "    subset_data = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    return get_num_tokens(subset_data, tokenizer) / num_samples\n",
    "\n",
    "# Calculate average tokens for each dataset\n",
    "datasets = {\n",
    "    '10BT': finewebedu_10bt,\n",
    "    '1BT': finewebedu_1bt,\n",
    "    '100MT': finewebedu_100mt\n",
    "}\n",
    "\n",
    "stats_data = []\n",
    "for name, dataset in datasets.items():\n",
    "    num_examples = get_num_examples(dataset)\n",
    "    avg_gpt2_tokens = calc_avg_tokens(dataset, gpt_tokenizer)\n",
    "    avg_llama3_tokens = calc_avg_tokens(dataset, llama3_tokenizer)\n",
    "    \n",
    "    stats_data.append({\n",
    "        'Dataset': name,\n",
    "        'Examples': format_int(num_examples),\n",
    "        'GPT-2 Tokens': format_int(num_examples * avg_gpt2_tokens),\n",
    "        'Llama-3 Tokens': format_int(num_examples * avg_llama3_tokens)\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(stats_data).set_index(['Dataset'])\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we are getting 100%, 10%, and 1% of the dataset, i.e. roughly 10B, 1B, and 100M GPT-2 training tokens, respectively. Let's upload the processed dataset to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-10bt\"\n",
    "finewebedu_10bt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-1bt\"\n",
    "finewebedu_1bt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-100mt\"\n",
    "finewebedu_100mt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also pre-tokenize the dataset and store it in the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=104): 100%|██████████| 9672101/9672101 [04:37<00:00, 34826.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 9672101\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.utils import tokenize\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "fineweb_edu_10bt_tok = finewebedu_10bt.map(\n",
    "    lambda x: tokenize(x[\"text\"], tokenizer, max_length=1025),\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=finewebedu_10bt.column_names\n",
    ")\n",
    "\n",
    "print(fineweb_edu_10bt_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/100 shards):   0%|          | 39000/9672101 [00:00<00:26, 369040.31 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (100/100 shards): 100%|██████████| 9672101/9672101 [00:26<00:00, 360586.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Create a DatasetDict with train split\n",
    "fineweb_edu_10bt_tok_dict = DatasetDict({\"train\": fineweb_edu_10bt_tok})\n",
    "\n",
    "# Save the DatasetDict to disk\n",
    "fineweb_edu_10bt_tok_dict.save_to_disk(\"/alloc/huggingface/mikasenghaas/fineweb-edu-10bt-tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.63ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.16ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.47ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.28ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.34ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.70ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.55ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.07ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.46ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.62ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.54ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.16ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.35ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.51ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.36ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.67ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.40ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.47ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.46ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 22.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.45ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.20ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 22.93ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.45ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.12ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 22.95ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.21ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.38ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 22.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.40ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.28ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.43ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.20ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.13ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.15ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.38ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.02ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 97/97 [00:04<00:00, 23.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 100/100 [13:38<00:00,  8.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed to https://huggingface.co/datasets/mikasenghaas/fineweb-edu-10bt-tokenized\n"
     ]
    }
   ],
   "source": [
    "# Push tokenized version to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-10bt-tokenized\"\n",
    "fineweb_edu_10bt_tok.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
