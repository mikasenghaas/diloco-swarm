{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Data\n",
    "\n",
    "This notebook contains code for the data in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from src.utils import format_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText 2\n",
    "\n",
    "For now, we will usoe a tiny dataset `Salesforce/wikitext/wikitext-2-raw-v1`. It has a train, validation and test split that consist of 37K, 1.8K and 2.2K examples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText 2\n",
    "wiki = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", cache_dir=\"/workspace/huggingface\")\n",
    "train_wiki, val_wiki, test_wiki = wiki[\"train\"], wiki[\"validation\"], wiki[\"test\"]\n",
    "\n",
    "print(f\"Loaded {len(train_wiki)/1e3:.1f}K training, {len(val_wiki)/1e3:.1f}K validation and {len(test_wiki)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single example just has a `text` field, which contains a single line of text. They are parsed from high quality Wikipedia articles. We can already see that there are loads of empty lines and other artiffacts like headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "for example in train_wiki.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove empty lines, headlines, and trailing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_empty_text(examples: Dict[str, Any]) -> bool:\n",
    "    return examples[\"text\"] != \"\"\n",
    "\n",
    "def non_headline(examples: Dict[str, Any]) -> bool:\n",
    "    return not examples[\"text\"].startswith(\" = \")\n",
    "\n",
    "def strip_headline(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    examples[\"text\"] = examples[\"text\"].lstrip().rstrip()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_processed = train_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "val_wiki_processed = val_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "test_wiki_processed = test_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "\n",
    "print(f\"Processed {len(train_wiki_processed)/1e3:.1f}K training, {len(val_wiki_processed)/1e3:.1f}K validation and {len(test_wiki_processed)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_wiki_processed.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's get some statistics on the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# Llama 2 tokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test'],\n",
    "    'Examples': map(format_int, [get_num_examples(train_wiki_processed), get_num_examples(val_wiki_processed), get_num_examples(test_wiki_processed)]),\n",
    "    'GPT-2 Tokens': map(format_int, [get_num_tokens(train_wiki_processed, gpt2_tokenizer), get_num_tokens(val_wiki_processed, gpt2_tokenizer), get_num_tokens(test_wiki_processed, gpt2_tokenizer)]),\n",
    "    'Llama-3 Tokens': map(format_int, [get_num_tokens(train_wiki_processed, llama3_tokenizer), get_num_tokens(val_wiki_processed, llama3_tokenizer), get_num_tokens(test_wiki_processed, llama3_tokenizer)])\n",
    "}).set_index('Split')\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's push the processed datasets to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "data = DatasetDict({\n",
    "    'train': train_wiki_processed,\n",
    "    'validation': val_wiki_processed,\n",
    "    'test': test_wiki_processed\n",
    "})\n",
    "\n",
    "repo_name = \"wikitext-2\"\n",
    "data.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinewebEdu\n",
    "\n",
    "The [FinewebEdu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) dataset is a large-scale pre-training dataset developed by the Hugging Face team. The smaller version consists of 1.3T high-quality tokens that have been filtered for quality using Llama 2 70B\n",
    "\n",
    "We are going to use the 10BT version of the dataset:\n",
    "- 9.67M Examples\n",
    "- 10.1B (GPT-2) Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinewebEdu (10BT)\n",
    "finewebedu_10bt = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\", cache_dir=\"/workspace/huggingface\")\n",
    "\n",
    "print(f\"Loaded {len(finewebedu_10bt)/1e6:.1f}M training examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset: Dataset) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Split dataset into 80% train, 10% eval, 10% test.\"\"\"\n",
    "    train_test_dict = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "    train_dataset = train_test_dict['train']\n",
    "    val_test_dict = train_test_dict['test'].train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
    "    eval_dataset = val_test_dict['train']\n",
    "    test_dataset = val_test_dict['test']\n",
    "\n",
    "    return train_dataset, eval_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 12.5% of FinewebEdu 10BT (1.25BT)\n",
    "finewebedu_125pct = finewebedu_10bt.shuffle(seed=42).select(range(int(len(finewebedu_10bt) * 0.125)))\n",
    "train_dataset, eval_dataset, test_dataset = train_val_test_split(finewebedu_125pct)\n",
    "\n",
    "finewebedu_1bt = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': eval_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(f\"Sampled {len(finewebedu_1bt['train'])/1e3:.1f}K training, {len(finewebedu_1bt['validation'])/1e3:.1f}K validation and {len(finewebedu_1bt['test'])/1e3:.1f}K test examples in 12% of 10BT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10% of FinewebEdu 1.25BT\n",
    "finewebedu_1_25pct = finewebedu_125pct.shuffle(seed=42).select(range(int(len(finewebedu_125pct) * 0.1)))\n",
    "train_dataset, eval_dataset, test_dataset = train_val_test_split(finewebedu_1_25pct)\n",
    "\n",
    "finewebedu_100mt = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': eval_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(f\"Sampled {len(finewebedu_100mt['train'])/1e3:.1f}K training, {len(finewebedu_100mt['validation'])/1e3:.1f}K validation and {len(finewebedu_100mt['test'])/1e3:.1f}K test examples in 12.5% of 10BT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# GPT-2 tokenizer\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Function to calculate average tokens per example\n",
    "def calc_avg_tokens(dataset, tokenizer, subset=0.1):\n",
    "    subset_size = int(len(dataset) * subset)\n",
    "    subset_data = dataset.shuffle(seed=42).select(range(subset_size))\n",
    "    return get_num_tokens(subset_data, tokenizer) / subset_size\n",
    "\n",
    "# Calculate average tokens for each dataset\n",
    "datasets = {\n",
    "    '1BT': finewebedu_1bt,\n",
    "    '100MT': finewebedu_100mt\n",
    "}\n",
    "\n",
    "stats_data = []\n",
    "avg_gpt2_tokens = calc_avg_tokens(finewebedu_100mt['train'], gpt_tokenizer)\n",
    "avg_llama3_tokens = calc_avg_tokens(finewebedu_100mt['train'], llama3_tokenizer)\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    for split in tqdm(dataset.keys()):\n",
    "        split_data = dataset[split]\n",
    "        num_examples = get_num_examples(split_data)\n",
    "        \n",
    "        stats_data.append({\n",
    "            'Dataset': name,\n",
    "            'Split': split,\n",
    "            'Examples': format_int(num_examples),\n",
    "            'GPT-2 Tokens': format_int(num_examples * avg_gpt2_tokens),\n",
    "            'Llama-3 Tokens': format_int(num_examples * avg_llama3_tokens)\n",
    "        })\n",
    "\n",
    "stats = pd.DataFrame(stats_data).set_index(['Dataset', 'Split'])\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we are getting 10%, and 1% of the dataset, i.e. roughly 1B and 100M GPT-2 training tokens, respectively. Let's upload the processed dataset to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-100mt\"\n",
    "finewebedu_100mt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-1bt\"\n",
    "finewebedu_1bt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
