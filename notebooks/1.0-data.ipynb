{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Data\n",
    "\n",
    "This notebook contains code for the data in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import disable_progress_bar\n",
    "from src.utils import get_dataset, get_tokenizer, non_empty_text, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "For now, we will use a tiny dataset `Salesforce/wikitext/wikitext-2-raw-v1`, which consists of 37K training, 3.7K validation and 4.3K test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText 2\n",
    "wiki = get_dataset( \"Salesforce/wikitext\",  \"wikitext-2-raw-v1\")\n",
    "train_wiki, val_wiki, test_wiki = wiki[\"train\"], wiki[\"validation\"], wiki[\"test\"]\n",
    "\n",
    "print(f\"Loaded {len(train_wiki)/1e3:.1f}K training, {len(val_wiki)/1e3:.1f}K validation and {len(test_wiki)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "for example in train_wiki.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single example just has a `text` field, which contains a single line of text. They are parsed from high quality Wikipedia articles. We can already see that there are loads of empty lines and other artiffacts like headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset\n",
    "\n",
    "We are going to remove empty lines, headlines and finally tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_empty_text(examples: Dict[str, Any]) -> bool:\n",
    "    return examples[\"text\"] != \"\"\n",
    "\n",
    "def non_headline(examples: Dict[str, Any]) -> bool:\n",
    "    return not examples[\"text\"].startswith(\" = \")\n",
    "\n",
    "def tokenize(examples: Dict[str, Any], tokenizer: AutoTokenizer, max_length: int) -> Dict[str, Any]:\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "batch_size = 32\n",
    "fn_kwargs = {\"tokenizer\": tokenizer, \"max_length\": 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_processed = train_wiki.filter(non_empty_text).filter(non_headline).map(tokenize, batched=True, batch_size=32, fn_kwargs=fn_kwargs).remove_columns([\"text\"])\n",
    "val_wiki_processed = val_wiki.filter(non_empty_text).filter(non_headline).map(tokenize, batched=True, batch_size=32, fn_kwargs=fn_kwargs).remove_columns([\"text\"])\n",
    "test_wiki_processed = test_wiki.filter(non_empty_text).filter(non_headline).map(tokenize, batched=True, batch_size=32, fn_kwargs=fn_kwargs).remove_columns([\"text\"])\n",
    "\n",
    "print(f\"Processed {len(train_wiki_processed)/1e3:.1f}K training, {len(val_wiki_processed)/1e3:.1f}K validation and {len(test_wiki_processed)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_wiki_processed.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    return {\n",
    "        'input_ids': torch.stack([torch.tensor(example['input_ids'][:-1]) for example in batch]),\n",
    "        'attention_mask': torch.stack([torch.tensor(example['attention_mask'][:-1]) for example in batch]),\n",
    "        'labels': torch.stack([torch.tensor(example['input_ids'][1:]) for example in batch]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_loader = DataLoader(train_wiki_processed, batch_size=batch_size, collate_fn=collate_fn)\n",
    "wiki_val_loader = DataLoader(val_wiki_processed, batch_size=batch_size, collate_fn=collate_fn)\n",
    "wiki_test_loader = DataLoader(test_wiki_processed, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Loaded {len(wiki_train_loader)} training, {len(wiki_val_loader)} validation and {len(wiki_test_loader)} test batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(wiki_train_loader))\n",
    "for key, value in batch.items():\n",
    "    print(key, value.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
