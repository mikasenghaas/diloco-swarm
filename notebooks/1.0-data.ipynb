{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Data\n",
    "\n",
    "This notebook contains code for the data in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from src.utils import format_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorizing Dataset\n",
    "\n",
    "We will create a single sample dummy dataset to test that we can overfit a single sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with a single example\n",
    "sentence = \"I am a large language model and I can memorize this sentence.\"\n",
    "dataset = Dataset.from_dict({\"text\": [sentence]})\n",
    "memorize = DatasetDict({\"train\": dataset, \"validation\": dataset, \"test\": dataset})\n",
    "\n",
    "print(f\"Created a dataset with a single example in train, validation and test: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "repo_name = \"memorize\"\n",
    "memorize.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText 2\n",
    "\n",
    "For now, we will usoe a tiny dataset `Salesforce/wikitext/wikitext-2-raw-v1`. It has a train, validation and test split that consist of 37K, 1.8K and 2.2K examples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText 2\n",
    "wiki = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", cache_dir=\"/workspace/huggingface\")\n",
    "train_wiki, val_wiki, test_wiki = wiki[\"train\"], wiki[\"validation\"], wiki[\"test\"]\n",
    "\n",
    "print(f\"Loaded {len(train_wiki)/1e3:.1f}K training, {len(val_wiki)/1e3:.1f}K validation and {len(test_wiki)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single example just has a `text` field, which contains a single line of text. They are parsed from high quality Wikipedia articles. We can already see that there are loads of empty lines and other artiffacts like headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "for example in train_wiki.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove empty lines, headlines, and trailing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_empty_text(examples: Dict[str, Any]) -> bool:\n",
    "    return examples[\"text\"] != \"\"\n",
    "\n",
    "def non_headline(examples: Dict[str, Any]) -> bool:\n",
    "    return not examples[\"text\"].startswith(\" = \")\n",
    "\n",
    "def strip_headline(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    examples[\"text\"] = examples[\"text\"].lstrip().rstrip()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_processed = train_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "val_wiki_processed = val_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "test_wiki_processed = test_wiki.filter(non_empty_text).filter(non_headline).map(strip_headline)\n",
    "\n",
    "print(f\"Processed {len(train_wiki_processed)/1e3:.1f}K training, {len(val_wiki_processed)/1e3:.1f}K validation and {len(test_wiki_processed)/1e3:.1f}K test examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_wiki_processed.take(5):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's get some statistics on the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# Llama 2 tokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test'],\n",
    "    'Examples': map(format_int, [get_num_examples(train_wiki_processed), get_num_examples(val_wiki_processed), get_num_examples(test_wiki_processed)]),\n",
    "    'GPT-2 Tokens': map(format_int, [get_num_tokens(train_wiki_processed, gpt2_tokenizer), get_num_tokens(val_wiki_processed, gpt2_tokenizer), get_num_tokens(test_wiki_processed, gpt2_tokenizer)]),\n",
    "    'Llama-3 Tokens': map(format_int, [get_num_tokens(train_wiki_processed, llama3_tokenizer), get_num_tokens(val_wiki_processed, llama3_tokenizer), get_num_tokens(test_wiki_processed, llama3_tokenizer)])\n",
    "}).set_index('Split')\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's push the processed datasets to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "data = DatasetDict({\n",
    "    'train': train_wiki_processed,\n",
    "    'validation': val_wiki_processed,\n",
    "    'test': test_wiki_processed\n",
    "})\n",
    "\n",
    "repo_name = \"wikitext-2\"\n",
    "data.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinewebEdu\n",
    "\n",
    "The [FinewebEdu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) dataset is a large-scale pre-training dataset developed by the Hugging Face team. The smaller version consists of 1.3T high-quality tokens that have been filtered for quality using Llama 2 70B\n",
    "\n",
    "We are going to use the 10BT version of the dataset which contains 9.67M samples, corresponding to roughly 10B GPT-2 training tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinewebEdu (10BT)\n",
    "finewebedu_10bt = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\", cache_dir=\"/workspace/huggingface\")\n",
    "\n",
    "print(f\"Loaded {len(finewebedu_10bt)/1e6:.1f}M training examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10% of FinewebEdu 10BT (1BT)\n",
    "finewebedu_1bt = finewebedu_10bt.shuffle(seed=42).select(range(int(len(finewebedu_10bt) * 0.1)))\n",
    "\n",
    "print(f\"Sampled {len(finewebedu_1bt)/1e6:.1f}M training examples (10% of 10BT).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10% of FinewebEdu 1.25BT\n",
    "finewebedu_100mt = finewebedu_10bt.shuffle(seed=42).select(range(int(len(finewebedu_1bt) * 0.1)))\n",
    "\n",
    "print(f\"Sampled {len(finewebedu_100mt)/1e6:.1f}M training examples (10% of 1BT).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "get_num_examples = lambda dataset: len(dataset)\n",
    "get_num_tokens = lambda dataset, tokenizer: sum(len(tokenizer.encode(example['text'])) for example in dataset)\n",
    "\n",
    "# GPT-2 tokenizer\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Function to calculate average tokens per example\n",
    "def calc_avg_tokens(dataset, tokenizer, num_samples=10000):\n",
    "    subset_data = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    return get_num_tokens(subset_data, tokenizer) / num_samples\n",
    "\n",
    "# Calculate average tokens for each dataset\n",
    "datasets = {\n",
    "    '10BT': finewebedu_10bt,\n",
    "    '1BT': finewebedu_1bt,\n",
    "    '100MT': finewebedu_100mt\n",
    "}\n",
    "\n",
    "stats_data = []\n",
    "for name, dataset in datasets.items():\n",
    "    num_examples = get_num_examples(dataset)\n",
    "    avg_gpt2_tokens = calc_avg_tokens(dataset, gpt_tokenizer)\n",
    "    avg_llama3_tokens = calc_avg_tokens(dataset, llama3_tokenizer)\n",
    "    \n",
    "    stats_data.append({\n",
    "        'Dataset': name,\n",
    "        'Examples': format_int(num_examples),\n",
    "        'GPT-2 Tokens': format_int(num_examples * avg_gpt2_tokens),\n",
    "        'Llama-3 Tokens': format_int(num_examples * avg_llama3_tokens)\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(stats_data).set_index(['Dataset'])\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we are getting 100%, 10%, and 1% of the dataset, i.e. roughly 10B, 1B, and 100M GPT-2 training tokens, respectively. Let's upload the processed dataset to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-10bt\"\n",
    "finewebedu_10bt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-1bt\"\n",
    "finewebedu_1bt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "repo_name = \"fineweb-edu-100mt\"\n",
    "finewebedu_100mt.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Pushed to https://huggingface.co/datasets/mikasenghaas/{repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
