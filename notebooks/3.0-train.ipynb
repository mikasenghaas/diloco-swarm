{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Training\n",
    "\n",
    "This notebook explores some aspects of training a model, and optimising for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a reasonably sized model for our tiny benchmark experiments and create a dummy batch that is representative in size of the actual workflows during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 : AutoModelForCausalLM = AutoModelForCausalLM.from_pretrained(\"mikasenghaas/gpt2-124m-fresh\").to(\"cuda\")\n",
    "\n",
    "print(f\"Loaded model with {gpt2.num_parameters()/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micro batch\n",
    "B, L, V = 8, 1024, 50257\n",
    "x = torch.randint(0, V, (B, L+1), dtype=torch.long).to(\"cuda\")\n",
    "batch = {\"input_ids\": x[:, :-1], \"attention_mask\": torch.ones_like(x[:, :-1]), \"labels\": x[:, 1:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation\n",
    "\n",
    "Modern LLM training formulas often require large batches, e.g. 512 batches with a sequence length of 1024. This is a total of ~0.5M tokens per step which is out of memory, even for powerful GPUs. Therefore, we use gradient accumulation to simulate a larger batch size. We simply set a true `micro_batch_size` and accumulate the gradients over `batch_size // micro_batch_size` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "\n",
    "PyTorch offers `torch.compile` which can compile your model into a static graph, and can lead to significant performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision\n",
    "\n",
    "We can speed up training by using lower precision for model weights, activations and gradients. PyTorch offers a one-liner to set the internal precision used in matrix multiplications (most used operation in Transformers), as well as a context manager to use lower precision for activations and gradients.\n",
    "\n",
    "Let's investigate the theoretical speed-ups we can expect from both of these techniques on a GPT-2 (124M) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"highest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "outputs = gpt2(**batch)\n",
    "outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    outputs = gpt2(**batch)\n",
    "outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    outputs = gpt2(**batch)\n",
    "outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "outputs = gpt2(**batch)\n",
    "outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    outputs = gpt2(**batch)\n",
    "outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    outputs = gpt2(**batch)\n",
    "outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slowest Run (`precision=\"highest\"` and `dtype=torch.float32`): 256ms\n",
    "Fastest Run (`precision=\"high\"` and `dtype=torch.bfloat16`): 143ms\n",
    "\n",
    "This is a ~1.8x speedup. We also see that the difference between `float16` and `bfloat16` is very small, so we prefer `bfloat16` because it has no precision loss. Further, we also get to 145ms with dtype `bfloat16` and `precision=\"highest\"`, so it seems that in this case precision `'high'` is redundant (i.e. it sets less precision where it is already set)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
