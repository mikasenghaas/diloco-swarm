{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Training\n",
    "\n",
    "This notebook explores some aspects of training a model, and optimising for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "from src.train.baseline import train, sample\n",
    "from src.utils import get_device, get_model, get_tokenizer, get_dataset, get_dataloader, get_micro_dataloader, tokenize, get_optimizer, get_scheduler\n",
    "from src.config import ModelConfig, TrainConfig, DataConfig, SampleConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a small model for debugging and running some tests. We'll also create a dummy batch that is representative in size of the actual workflows during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_config = ModelConfig(n_layer=12, n_head=12, n_embd=768) # GPT-2 Small (124M)\n",
    "train_config = TrainConfig(max_steps=100, max_epochs=-1, micro_batch_size=1, batch_size=1)\n",
    "data_config = DataConfig(path=\"mikasenghaas/memorize\", seq_length=32)\n",
    "sample_config = SampleConfig(num_samples=1)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = get_model(model_config)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = get_tokenizer()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataloader\n",
    "dataset = get_dataset(data_config, split=\"train\")\n",
    "data = dataset.map(lambda examples: tokenize(examples[\"text\"], tokenizer, max_length=data_config.seq_length+1, return_tensors=None))\n",
    "dataloader = get_dataloader(data, batch_size=1, shuffle=False, cycle=True)\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "model.to(device)\n",
    "optimizer = get_optimizer(model, train_config.optimizer)\n",
    "scheduler = get_scheduler(optimizer, train_config.scheduler)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "step = 0\n",
    "while step < train_config.max_steps:\n",
    "    batch = next(dataloader)\n",
    "    batchloader = get_micro_dataloader(batch, micro_batch_size=1)\n",
    "    outputs = train(step, model, batchloader, loss_fn, optimizer, scheduler, train_config, device)\n",
    "\n",
    "    if (step+1) % 10 == 0 or step == 0:\n",
    "        print(f\"Step: {step+1}, Loss: {outputs.loss:.4f} ({sample(model, tokenizer, sample_config, device=device)[0]})\")\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision\n",
    "\n",
    "We can speed up training by using lower precision for model weights, activations and gradients. PyTorch offers a one-liner to set the internal precision used in matrix multiplications (most used operation in Transformers), as well as a context manager to use lower precision for activations and gradients.\n",
    "\n",
    "Let's investigate the theoretical speed-ups we can expect from both of these techniques on a GPT-2 (124M) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micro batch\n",
    "B, L, V = 4, 1024, 50257\n",
    "x = torch.randint(0, V, (B, L+1), dtype=torch.long).to(\"cuda\")\n",
    "batch = {\n",
    "    \"input_ids\": x[:, :-1],\n",
    "    \"attention_mask\": torch.ones_like(x[:, :-1]),\n",
    "    \"labels\": x[:, 1:]\n",
    "}\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT2 model\n",
    "model_config = ModelConfig(n_layer=12, n_head=12, n_embd=768)\n",
    "model = get_model(model_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Precision + Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"highest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "logits = model.forward(input_ids=batch[\"input_ids\"])\n",
    "loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Precision + Float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    logits = model.forward(input_ids=batch[\"input_ids\"])\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Precision + BFloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    logits = model.forward(input_ids=batch[\"input_ids\"])\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Precision + Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "logits = model.forward(input_ids=batch[\"input_ids\"])\n",
    "loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Precision + Float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    logits = model.forward(input_ids=batch[\"input_ids\"])\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Precision + BFloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    logits = model.forward(input_ids=batch[\"input_ids\"])\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a ~2.8x speedup. We also see that the difference between `float16` and `bfloat16` is very small, so we prefer `bfloat16` because it has no precision loss. Further, we also get to 145ms with dtype `bfloat16` and `precision=\"highest\"`, so it seems that in this case precision `'high'` is redundant (i.e. it sets less precision where it is already set).\n",
    "\n",
    "- Slowest Run (`precision=\"highest\"` and `dtype=torch.float32`): 207ms\n",
    "- Fastest Run (`precision=\"high\"` and `dtype=torch.bfloat16`): 74ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "\n",
    "PyTorch offers `torch.compile` which can compile your model into a static graph, and can lead to significant performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
