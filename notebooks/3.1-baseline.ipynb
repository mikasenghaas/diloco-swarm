{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Training\n",
    "\n",
    "This notebook explores some aspects of training a model, and optimising for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "from src.utils import get_device, get_model, get_tokenizer, get_dataloader, get_micro_dataloader, tokenize, get_optimizer, get_scheduler\n",
    "from src.model import Model\n",
    "from src.train.baseline import train, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a small model for debugging and running some tests. We'll also create a dummy batch that is representative in size of the actual workflows during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with a single example\n",
    "sentence = \"I am a large language model and I can memorize this sentence.\"\n",
    "dataset = Dataset.from_dict({\"text\": [sentence]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 (9M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34343e2b3d5e4af49cee8a8f19637e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: 10.3590 (I am I I I I I I I IHD model lying model lying model lyingmaz lyingmaz model lyingmaz model lyingmaz model lyingmaz model lyingmazmazmazmazmazmazmazmazmazmazmazmazmazmazsuppmazsuppmazsuppmazsupp)\n",
      "Step: 10, Loss: 4.7743 (I am a large model and I large and I can memorize this sentence.)\n",
      "Step: 20, Loss: 0.8941 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 30, Loss: 0.0897 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 40, Loss: 0.0101 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 50, Loss: 0.0027 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 60, Loss: 0.0013 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 70, Loss: 0.0009 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 80, Loss: 0.0007 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 90, Loss: 0.0006 (I am a large language model and I can memorize this sentence.)\n",
      "Step: 100, Loss: 0.0005 (I am a large language model and I can memorize this sentence.)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_name = \"mikasenghaas/llama2-9m-fresh\"\n",
    "model : Model = get_model(model_name)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer : AutoTokenizer = get_tokenizer(model_name)\n",
    "bos, eos = tokenizer.bos_token_id, tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataloader\n",
    "seq_length = 32\n",
    "data = dataset.map(lambda examples: tokenize(examples[\"text\"], tokenizer, max_length=seq_length+1, return_tensors=None))\n",
    "dataloader = get_dataloader(data, batch_size=1, shuffle=False, cycle=True)\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "model.to(device)\n",
    "step, max_steps = 0, 100\n",
    "optimizer = get_optimizer(model, lr=3e-3, weight_decay=0.0, betas=(0.9, 0.95))\n",
    "scheduler = get_scheduler(optimizer, num_steps=None, warmup_steps=None, num_cycles=None, min_lr_factor=None, last_epoch=None, enable=False)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "while step < max_steps:\n",
    "    batch = next(dataloader)\n",
    "    batchloader = get_micro_dataloader(batch, micro_batch_size=1)\n",
    "    outputs = train(step, model, batchloader, loss_fn, optimizer, scheduler, device, \"float32\", 1.0)\n",
    "\n",
    "    if (step+1) % 10 == 0 or step == 0:\n",
    "        print(f\"Step: {step+1}, Loss: {outputs.loss:.4f} ({sample(model, tokenizer, 'I am', 50, device=device)})\")\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 (124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d8a817a09342c7af5b02dc27c77751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: 10.6368 (I am  a a a aizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeizeize)\n",
      "Step: 10, Loss: 0.1532 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 20, Loss: 0.0044 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 30, Loss: 0.0013 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 40, Loss: 0.0006 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 50, Loss: 0.0004 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 60, Loss: 0.0003 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 70, Loss: 0.0002 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 80, Loss: 0.0002 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 90, Loss: 0.0001 (I am  a large language model and I can memorize this sentence.)\n",
      "Step: 100, Loss: 0.0001 (I am  a large language model and I can memorize this sentence.)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_name = \"mikasenghaas/gpt2-124m-fresh\"\n",
    "model : Model = get_model(model_name)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer : AutoTokenizer = get_tokenizer(model_name)\n",
    "bos, eos = tokenizer.bos_token_id, tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataloader\n",
    "seq_length = 32\n",
    "data = dataset.map(lambda examples: tokenize(examples[\"text\"], tokenizer, max_length=seq_length+1, return_tensors=None))\n",
    "dataloader = get_dataloader(data, batch_size=1, shuffle=False, cycle=True)\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "model.to(device)\n",
    "step, max_steps = 0, 100\n",
    "optimizer = get_optimizer(model, lr=3e-4, weight_decay=0.0, betas=(0.9, 0.95))\n",
    "scheduler = get_scheduler(optimizer, num_steps=None, warmup_steps=None, num_cycles=None, min_lr_factor=None, last_epoch=None, enable=False)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "while step < max_steps:\n",
    "    batch = next(dataloader)\n",
    "    batchloader = get_micro_dataloader(batch, micro_batch_size=1)\n",
    "    outputs = train(step, model, batchloader, loss_fn, optimizer, scheduler, device, \"float32\", 1.0)\n",
    "\n",
    "    if (step+1) % 10 == 0 or step == 0:\n",
    "        print(f\"Step: {step+1}, Loss: {outputs.loss:.4f} ({sample(model, tokenizer, 'I am', 50, device=device)})\")\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision\n",
    "\n",
    "We can speed up training by using lower precision for model weights, activations and gradients. PyTorch offers a one-liner to set the internal precision used in matrix multiplications (most used operation in Transformers), as well as a context manager to use lower precision for activations and gradients.\n",
    "\n",
    "Let's investigate the theoretical speed-ups we can expect from both of these techniques on a GPT-2 (124M) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micro batch\n",
    "B, L, V = 4, 1024, 50257\n",
    "x = torch.randint(0, V, (B, L+1), dtype=torch.long).to(\"cuda\")\n",
    "batch = {\n",
    "    \"input_ids\": x[:, :-1],\n",
    "    \"attention_mask\": torch.ones_like(x[:, :-1]),\n",
    "    \"labels\": x[:, 1:]\n",
    "}\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 (9M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 (124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT2 model\n",
    "model_name = \"mikasenghaas/gpt2-124m-fresh\"\n",
    "model : Model = get_model(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Precision + Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"highest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-n 5 -r 5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits = gpt2(**batch)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mloss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m].reshape(-1))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mloss.backward()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/miniconda/envs/swarm/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/workspace/miniconda/envs/swarm/lib/python3.10/site-packages/IPython/core/magics/execution.py:1199\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1197\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m all_runs \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n\u001b[1;32m   1201\u001b[0m worst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n",
      "File \u001b[0;32m/workspace/miniconda/envs/swarm/lib/python3.10/timeit.py:206\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    204\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 206\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/workspace/miniconda/envs/swarm/lib/python3.10/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt2' is not defined"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "logits = gpt2(**batch)\n",
    "loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Precision + Float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    logits = gpt2(**batch)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Precision + BFloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    logits = gpt2(**batch)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Precision + Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "logits = gpt2(**batch)\n",
    "loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Precision + Float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    logits = gpt2(**batch)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Precision + BFloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    logits = gpt2(**batch)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), batch[\"labels\"].reshape(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slowest Run (`precision=\"highest\"` and `dtype=torch.float32`): 256ms\n",
    "Fastest Run (`precision=\"high\"` and `dtype=torch.bfloat16`): 143ms\n",
    "\n",
    "This is a ~1.8x speedup. We also see that the difference between `float16` and `bfloat16` is very small, so we prefer `bfloat16` because it has no precision loss. Further, we also get to 145ms with dtype `bfloat16` and `precision=\"highest\"`, so it seems that in this case precision `'high'` is redundant (i.e. it sets less precision where it is already set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "\n",
    "PyTorch offers `torch.compile` which can compile your model into a static graph, and can lead to significant performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
