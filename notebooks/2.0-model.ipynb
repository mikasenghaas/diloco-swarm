{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Model\n",
    "\n",
    "This notebook describes and prepares the models used in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import ModelConfig\n",
    "from src.model import GPT2, GPT2Config\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Family\n",
    "\n",
    "We want to use a family of LLMS for our experiments. A good candidate is GPT-2:\n",
    "\n",
    "- Family of models with different, but not too large sizes (124M, 355M, 774M)\n",
    "- Open-source paper [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "- Open-source weights available on [Hugging Face](https://huggingface.co/openai-community/gpt2)\n",
    "- Custom minimal implementation in PyTorch available in [NanoGPT](https://github.com/karpathy/nanoGPT) and benchmarks on performance and validation on common benchmarks\n",
    "\n",
    "The only drawback seems to be that the tokenizer is a bit simplistic, but it will be good enough for our purposes. Let's get familar with the model family by loading its weights and running some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_num_params(config: GPT2Config):\n",
    "    non_layer_params = config.vocab_size * config.n_embd + config.block_size * config.n_embd + config.n_embd\n",
    "    layer_params = (4 * config.n_embd * config.n_embd) + (8 * config.n_embd * config.n_embd) + (2 * config.n_embd)\n",
    "    if config.bias:\n",
    "        non_layer_params += config.n_embd\n",
    "        layer_params += (4 * config.n_embd) + (8 * config.n_embd) + (2 * config.n_embd)\n",
    "    return config.n_layer * layer_params + non_layer_params\n",
    "\n",
    "models = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "cols = {\"Num. Layers\": \"n_layer\", \"Num. Heads\": \"n_head\", \"Embedding Dim.\": \"n_embd\"}\n",
    "rows = []\n",
    "for model_name in models:\n",
    "    config = AutoConfig.from_pretrained(f\"openai-community/{model_name}\")\n",
    "    filtered_config = {k: v for k, v in config.__dict__.items() if k in cols.values()}\n",
    "    row = [filtered_config[key] for key in cols.values()]\n",
    "    row.extend([int(get_gpt2_num_params(GPT2Config(**filtered_config)) / 1e6)])\n",
    "    rows.append(row)\n",
    "pd.DataFrame(rows, index=models, columns=list(cols.keys()) + [\"Num. Params (M)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we get models between 124M and 1.5B parameters which is perfect for our experiments. They only differ by the number of layers, heads and embedding dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 (124M) from HF\n",
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "hf_gpt2 = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded {hf_gpt2.config._name_or_path} with {hf_gpt2.num_parameters() / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence\n",
    "pipe = pipeline(\"text-generation\", model=hf_gpt2, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, max_new_tokens=10, device=\"cuda\")\n",
    "generated = pipe(\"Hello World!\")\n",
    "print(generated[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch GPT-2\n",
    "\n",
    "Based on the ~250LOC implementation of GPT-2 from [NanoGPT](https://github.com/karpathy/nanoGPT), we have a custom PyTorch model with functionality to load and save checkpoints from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom GPT-2 (PyTorch)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2 = GPT2(GPT2Config())\n",
    "\n",
    "print(f\"Loaded GPT-2 with {gpt2.num_parameters() / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(gpt2.generate(tokenizer.encode(\"Hello World!\", return_tensors=\"pt\"), 10)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From custom GPT-2 from pre-trained weights\n",
    "gpt2 = GPT2.from_hf(hf_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(gpt2.generate(tokenizer.encode(\"Hello World!\", return_tensors=\"pt\"), 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch <> HF Conversion\n",
    "\n",
    "The goal is so that we can arbitrarily convert between PyTorch and Hugging Face models. Let's test this by generating a sequence with the PyTorch model and then converting it to a Hugging Face model and generating a sequence with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local PyTorch model to Hugging Face model\n",
    "gpt2.save_pretrained(\"gpt2\")\n",
    "\n",
    "gpt2 = GPT2.from_pretrained(\"gpt2\")\n",
    "print(tokenizer.decode(gpt2.generate(tokenizer.encode(\"Hello World!\", return_tensors=\"pt\"), 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to HuggingFace Hub\n",
    "\n",
    "Finally, let's push fresh versions of all model sizes to Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push GPT-2 Small\n",
    "with open(\"configs/model/gpt2-small.toml\", \"r\") as f:\n",
    "    model_config = ModelConfig(**dict(map(lambda s: s.strip().split(\" = \"), f.readlines())))\n",
    "\n",
    "# Initialize fresh GPT-2 Small\n",
    "gpt2_small = GPT2(GPT2Config(**model_config.dict()))\n",
    "\n",
    "# Push to HF Hub\n",
    "repo_name = \"gpt2-small-fresh\"\n",
    "gpt2_small.push_to_hub(repo_name, use_auth_token=True)\n",
    "tokenizer.push_to_hub(repo_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push GPT-2 Medium\n",
    "with open(\"configs/model/gpt2-medium.toml\", \"r\") as f:\n",
    "    model_config = ModelConfig(**dict(map(lambda s: s.strip().split(\" = \"), f.readlines())))\n",
    "\n",
    "# Initialize fresh GPT-2 Small\n",
    "gpt2_medium = GPT2(GPT2Config(**model_config.dict()))\n",
    "\n",
    "# Push to HF Hub\n",
    "repo_name = \"gpt2-medium-fresh\"\n",
    "gpt2_medium.push_to_hub(repo_name, use_auth_token=True)\n",
    "tokenizer.push_to_hub(repo_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Medium\n",
    "with open(\"configs/model/gpt2-large.toml\", \"r\") as f:\n",
    "    model_config = ModelConfig(**dict(map(lambda s: s.strip().split(\" = \"), f.readlines())))\n",
    "\n",
    "# Initialize fresh GPT-2 Large\n",
    "gpt2_large = GPT2(GPT2Config(**model_config.dict()))\n",
    "\n",
    "# Push to HF Hub\n",
    "repo_name = \"gpt2-large-fresh\"\n",
    "gpt2_large.push_to_hub(repo_name, use_auth_token=True)\n",
    "tokenizer.push_to_hub(repo_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 XL\n",
    "with open(\"configs/model/gpt2-xl.toml\", \"r\") as f:\n",
    "    model_config = ModelConfig(**dict(map(lambda s: s.strip().split(\" = \"), f.readlines())))\n",
    "\n",
    "# Initialize fresh GPT-2 XL\n",
    "gpt2_xl = GPT2(GPT2Config(**model_config.dict()))\n",
    "\n",
    "# Push to HF Hub\n",
    "repo_name = \"gpt2-xl-fresh\"\n",
    "gpt2_xl.push_to_hub(repo_name, use_auth_token=True)\n",
    "tokenizer.push_to_hub(repo_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from HuggingFace Hub\n",
    "\n",
    "Let's test if we can load the models from the Hugging Face Hub again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mikasenghaas/gpt2-small-fresh\")\n",
    "gpt2_small = GPT2.from_pretrained(\"mikasenghaas/gpt2-small-fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(gpt2.generate(tokenizer.encode(\"Hello World!\", return_tensors=\"pt\"), 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT-2 with 162.32M parameters\n"
     ]
    }
   ],
   "source": [
    "from src.utils import get_model, get_sharded_model\n",
    "\n",
    "model_config = ModelConfig(n_layer=12, n_head=12, n_embd=768, parameter_sharing=False)\n",
    "model = get_model(model_config)\n",
    "print(f\"Loaded GPT-2 with {model.num_parameters() / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake world\n",
    "from src.world import World\n",
    "\n",
    "world0 = World(local_rank=0, world_size=2, debug=True)\n",
    "world1 = World(local_rank=1, world_size=2, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0: 81.16M parameters\n",
      "Shard 1: 81.16M parameters\n"
     ]
    }
   ],
   "source": [
    "# Shard model\n",
    "from copy import deepcopy\n",
    "\n",
    "shard0 = get_sharded_model(deepcopy(model), world0)\n",
    "shard1 = get_sharded_model(deepcopy(model), world1)\n",
    "\n",
    "print(f\"Shard 0: {shard0.num_parameters() / 1e6:.2f}M parameters\")\n",
    "print(f\"Shard 1: {shard1.num_parameters() / 1e6:.2f}M parameters\")\n",
    "assert shard0.num_parameters() + shard1.num_parameters() == model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the original GPT-2 model shares the weights for the embeddings and the LM head. In pipeline parallel training, this sharing is more difficult so we will default to not share parameters for simplicity and comparability between methods. However, this may make comparing this model difficult with GPT-2 baselines because of the different parameter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "input_ids = torch.randint(0, 50257, (1, 1024))\n",
    "model_out = model.forward(input_ids=input_ids)\n",
    "shard_out = shard1.forward(hidden_states=shard0.forward(input_ids=input_ids))\n",
    "\n",
    "assert torch.allclose(model_out, shard_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We have successfully sharded the model and get equivalent forward passes. Let's check the backward pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
