{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦™ Model\n",
    "\n",
    "This notebook contains code for the models in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from src.utils import get_model, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model: AutoModelForCausalLM) -> Dict:\n",
    "    \"\"\"Get the information about a model's architecture.\"\"\"\n",
    "    return {\n",
    "        \"num_params\": model.num_parameters(),\n",
    "        \"num_bytes\": model.num_parameters() * 4,\n",
    "        \"num_layers\": len(model.model.layers),\n",
    "        \"vocab_size\": model.config.vocab_size,\n",
    "        \"hidden_size\": model.config.hidden_size,\n",
    "        \"intermediate_size\": model.config.intermediate_size,\n",
    "        \"num_heads\": model.config.num_attention_heads,\n",
    "        \"head_dim\": model.config.head_dim\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_info(tokenizer: AutoTokenizer) -> Dict:\n",
    "    \"\"\"Get the information about a tokenizer's architecture.\"\"\"\n",
    "    return {\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "        \"special_tokens\": tokenizer.special_tokens_map,\n",
    "        \"model_max_length\": tokenizer.model_max_length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate_number(num: float) -> str:\n",
    "    \"\"\"Abbreviate a number to a string with M for millions or K for thousands.\"\"\"\n",
    "    if num >= 1e6:\n",
    "        return f\"{num/1e6:.1f}M\"\n",
    "    elif num >= 10e3:\n",
    "        return f\"{num/1e3:.1f}K\"\n",
    "    else:\n",
    "        return str(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "I am using randomly initialised Llama ðŸ¦™ models from the [PrimeIntellect](https://huggingface.co/PrimeIntellect) HuggingFace profile. They have fresh instances in four sizes:\n",
    "\n",
    "* [Llama 14M](https://huggingface.co/PrimeIntellect/llama-14m-fresh)\n",
    "* [Llama 60M](https://huggingface.co/PrimeIntellect/llama-60m-fresh)\n",
    "* [Llama 150M](https://huggingface.co/PrimeIntellect/llama-150m-fresh)\n",
    "* [Llama 1B](https://huggingface.co/PrimeIntellect/llama-1b-fresh)\n",
    "\n",
    "Let's load them, and check their size and architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 14M\n",
    "llama14m = get_model( \"PrimeIntellect/llama-14m-fresh\")\n",
    "llama14m_info = get_model_info(llama14m)\n",
    "\n",
    "print(f\"Loaded '{llama14m.config._name_or_path}' with {llama14m_info['num_params']/1e6:.2f}M parameters.\")\n",
    "\n",
    "llama14m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama60m = get_model( \"PrimeIntellect/llama-60m-fresh\")\n",
    "llama60m_info = get_model_info(llama60m)\n",
    "\n",
    "print(f\"Loaded '{llama60m.config._name_or_path}' with {llama60m_info['num_params']/1e6:.2f}M parameters.\")\n",
    "llama60m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a `LlamaForCausalLM` model, which is a HuggingFace class for decoder-only Transformers from the Llama family. Let's check the architectures of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {k: abbreviate_number(v) for k, v in llama14m_info.items()},\n",
    "    {k: abbreviate_number(v) for k, v in llama60m_info.items()}\n",
    "], index=[\"Llama 14M\", \"Llama 60M\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "The PrimeIntellect models do not come with a tokenizer, so I assume any Llama tokenizer will work. Also, Prime says they are using the Mistral 7B tokenizer internally, so I will also check that. There are multiple options:\n",
    "\n",
    "* [Llama 2 tokenizer](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "* [Llama 3 tokenizer](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "* [Mistral 7B](mistralai/Mistral-7B-v0.1)\n",
    "\n",
    "Let's load them and check their vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 2 7B tokenizer\n",
    "llama2_tokenizer = get_tokenizer(\"meta-llama/Llama-2-7b-hf\" )\n",
    "llama2_tokenizer_info = get_tokenizer_info(llama2_tokenizer)\n",
    "\n",
    "print(f\"Loaded '{llama2_tokenizer.name_or_path}' with {llama2_tokenizer_info['vocab_size']} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 3.2 1B tokenizer\n",
    "llama3_tokenizer = get_tokenizer(\"meta-llama/Llama-3.2-1B\")\n",
    "llama3_tokenizer_info = get_tokenizer_info(llama3_tokenizer)\n",
    "\n",
    "print(f\"Loaded '{llama3_tokenizer.name_or_path}' with {llama3_tokenizer_info['vocab_size']} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mistral 7B tokenizer\n",
    "mistral_tokenizer = get_tokenizer(\"mistralai/Mistral-7B-v0.1\")\n",
    "mistral_tokenizer_info = get_tokenizer_info(mistral_tokenizer)\n",
    "\n",
    "print(f\"Loaded '{mistral_tokenizer.name_or_path}' with {mistral_tokenizer_info['vocab_size']} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {k: abbreviate_number(v) if k != \"special_tokens\" else v for k, v in llama2_tokenizer_info.items()},\n",
    "    {k: abbreviate_number(v) if k != \"special_tokens\" else v for k, v in llama3_tokenizer_info.items()},\n",
    "    {k: abbreviate_number(v) if k != \"special_tokens\" else v for k, v in mistral_tokenizer_info.items()}\n",
    "], index=[\"Llama 2\", \"Llama 3\", \"Mistral 7B\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the vocabulary size for the Llama 2 tokenizer is identical to the input dimension of the Prime Intellect models, it is likely that they are based on the Llama 2 tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "The models do not produce reasonable outputs yet, as they are randomly initialised. Let's quickly verify this by sampling some outputs using a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=llama14m, tokenizer=llama2_tokenizer, pad_token_id=llama2_tokenizer.eos_token_id, device=\"cpu\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
