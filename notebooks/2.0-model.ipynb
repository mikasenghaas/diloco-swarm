{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦™ Model\n",
    "\n",
    "This notebook contains code for the models in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, LlamaForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prime Intellect's Llama 2 (14M)\n",
    "\n",
    "For debugging, I am using randomly initialised Llama ðŸ¦™ models from the [PrimeIntellect](https://huggingface.co/PrimeIntellect) HuggingFace profile. They have fresh instances in four sizes:\n",
    "\n",
    "* [Llama 14M](https://huggingface.co/PrimeIntellect/llama-14m-fresh)\n",
    "* [Llama 60M](https://huggingface.co/PrimeIntellect/llama-60m-fresh)\n",
    "* [Llama 150M](https://huggingface.co/PrimeIntellect/llama-150m-fresh)\n",
    "* [Llama 1B](https://huggingface.co/PrimeIntellect/llama-1b-fresh)\n",
    "\n",
    "We use the smallest model to check the architecture and push a copy to the HF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 14M (in reality its 9M)\n",
    "llama2_9m_fresh = AutoModelForCausalLM.from_pretrained(\"PrimeIntellect/llama-14m-fresh\")\n",
    "llama2_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "print(f\"Loaded '{llama2_9m_fresh.config._name_or_path}' with {llama2_9m_fresh.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{llama2_tokenizer.name_or_path}' with {llama2_tokenizer.vocab_size} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a `LlamaForCausalLM` model, which is a HuggingFace class for decoder-only Transformers from the Llama family. Let's check the architectures of the two models. The Llama 14M and 60M models have similar architectures. Importantly, their vocab size is 32K which matches the vocabulary size of the Llama 2 tokenizer. Let's load the tokenizer and check its size (`meta-llama/Llama-2-7b-hf`). We could also use Mistral's (`mistralai/Mistral-7B-v0.1`) tokenizer which is the exact same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are fresh instances, so we don't expect them to produce reasonable outputs. Let's sample some outputs using a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=llama2_9m_fresh, tokenizer=llama2_tokenizer, pad_token_id=llama2_tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, let's push the model to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HuggingFace Hub\n",
    "repo_name = \"llama2-9m-fresh\"\n",
    "llama2_9m_fresh.push_to_hub(repo_name, use_auth_token=True)\n",
    "llama2_tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 (124M)\n",
    "\n",
    "Next, let's also load a GPT-2 (124M) model which we will use to replicate the NanoGPT experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 124M\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(f\"Loaded '{gpt2.config._name_or_path}' with {gpt2.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{gpt2_tokenizer.name_or_path}' with {gpt2_tokenizer.vocab_size} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=gpt2, tokenizer=gpt2_tokenizer, pad_token_id=gpt2_tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push fresh instance to HuggingFace Hub\n",
    "gpt2_fresh = GPT2LMHeadModel(gpt2.config)\n",
    "gpt2_fresh.init_weights()\n",
    "\n",
    "repo_name = \"gpt2-124m-fresh\"\n",
    "gpt2_fresh.push_to_hub(repo_name, use_auth_token=True)\n",
    "gpt2_tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.2 (1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 3.2 1B \n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "llama32_1b = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "llama32_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded '{llama32_1b.config._name_or_path}' with {llama32_1b.num_parameters()/1e9:.2f}B parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=llama32_1b, tokenizer=llama32_tokenizer, pad_token_id=llama32_tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push fresh instance to HuggingFace Hub\n",
    "llama32_1b_fresh = LlamaForCausalLM(llama32_1b.config)\n",
    "llama32_1b_fresh.init_weights()\n",
    "\n",
    "# Push to HuggingFace Hub\n",
    "repo_name = \"llama32-1b-fresh\"\n",
    "llama32_1b_fresh.push_to_hub(repo_name, use_auth_token=True)\n",
    "llama32_tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_model, get_tokenizer\n",
    "from src.config import ModelConfig, TokenizerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 2 (9M)\n",
    "model_name = \"mikasenghaas/llama2-9m-fresh\"\n",
    "model = get_model(ModelConfig(name=model_name))\n",
    "tokenizer = get_tokenizer(TokenizerConfig(name=model_name))\n",
    "\n",
    "# Print model and tokenizer details\n",
    "print(f\"Loaded '{model.config._name_or_path}' with {model.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{tokenizer.name_or_path}' with {tokenizer.vocab_size} tokens.\\n\")\n",
    "\n",
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPT-2 124M\n",
    "model_name = \"mikasenghaas/gpt2-124m-fresh\"\n",
    "model = get_model(ModelConfig(name=model_name))\n",
    "tokenizer = get_tokenizer(TokenizerConfig(name=model_name))\n",
    "\n",
    "# Print model and tokenizer details\n",
    "print(f\"Loaded '{model.config._name_or_path}' with {model.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{tokenizer.name_or_path}' with {tokenizer.vocab_size} tokens.\\n\")\n",
    "\n",
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LLama 3.2 1B\n",
    "model_name = \"mikasenghaas/llama32-1b-fresh\"\n",
    "model = get_model(ModelConfig(name=model_name))\n",
    "tokenizer = get_tokenizer(TokenizerConfig(name=model_name))\n",
    "\n",
    "# Print model and tokenizer details\n",
    "print(f\"Loaded '{model.config._name_or_path}' with {model.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{tokenizer.name_or_path}' with {tokenizer.vocab_size} tokens.\\n\")\n",
    "\n",
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
