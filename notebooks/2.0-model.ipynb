{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦙 Model\n",
    "\n",
    "This notebook contains code for the models in this experiment suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, LlamaForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prime Intellect's Llama 2 (14M)\n",
    "\n",
    "For debugging, I am using randomly initialised Llama 🦙 models from the [PrimeIntellect](https://huggingface.co/PrimeIntellect) HuggingFace profile. They have fresh instances in four sizes:\n",
    "\n",
    "* [Llama 14M](https://huggingface.co/PrimeIntellect/llama-14m-fresh)\n",
    "* [Llama 60M](https://huggingface.co/PrimeIntellect/llama-60m-fresh)\n",
    "* [Llama 150M](https://huggingface.co/PrimeIntellect/llama-150m-fresh)\n",
    "* [Llama 1B](https://huggingface.co/PrimeIntellect/llama-1b-fresh)\n",
    "\n",
    "We use the smallest model to check the architecture and push a copy to the HF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'PrimeIntellect/llama-14m-fresh' with 9.77M parameters.\n",
      "Loaded 'meta-llama/Llama-2-7b-hf' with 32000 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 14M (in reality its 9M)\n",
    "llama2_9m_fresh = AutoModelForCausalLM.from_pretrained(\"PrimeIntellect/llama-14m-fresh\")\n",
    "llama2_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "print(f\"Loaded '{llama2_9m_fresh.config._name_or_path}' with {llama2_9m_fresh.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{llama2_tokenizer.name_or_path}' with {llama2_tokenizer.vocab_size} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a `LlamaForCausalLM` model, which is a HuggingFace class for decoder-only Transformers from the Llama family. Let's check the architectures of the two models. The Llama 14M and 60M models have similar architectures. Importantly, their vocab size is 32K which matches the vocabulary size of the Llama 2 tokenizer. Let's load the tokenizer and check its size (`meta-llama/Llama-2-7b-hf`). We could also use Mistral's (`mistralai/Mistral-7B-v0.1`) tokenizer which is the exact same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are fresh instances, so we don't expect them to produce reasonable outputs. Let's sample some outputs using a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, how are you? accept accept System accept System accept System acceptäll пробле'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=llama2_9m_fresh, tokenizer=llama2_tokenizer, pad_token_id=llama2_tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, let's push the model to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4381b64780e147188d553862ab510eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/39.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ead5804a0944e15b5be24f5d9ec9aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/llama2-9m-fresh\n"
     ]
    }
   ],
   "source": [
    "# Push to HuggingFace Hub\n",
    "repo_name = \"llama2-9m-fresh\"\n",
    "llama2_9m_fresh.push_to_hub(repo_name, use_auth_token=True)\n",
    "llama2_tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 (124M)\n",
    "\n",
    "Next, let's also load a GPT-2 (124M) model which we will use to replicate the NanoGPT experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'gpt2' with 124.44M parameters.\n",
      "Loaded 'gpt2' with 50257 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 124M\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(f\"Loaded '{gpt2.config._name_or_path}' with {gpt2.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{gpt2_tokenizer.name_or_path}' with {gpt2_tokenizer.vocab_size} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you? Do you need a bathroom?\"\\n\\nThe two'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=gpt2, tokenizer=gpt2_tokenizer, pad_token_id=gpt2_tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b626fc51ac441b9b7443b7d4f0a60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/gpt2-124m-fresh\n"
     ]
    }
   ],
   "source": [
    "# Push fresh instance to HuggingFace Hub\n",
    "gpt2_fresh = GPT2LMHeadModel(gpt2.config)\n",
    "gpt2_fresh.init_weights()\n",
    "\n",
    "repo_name = \"gpt2-124m-fresh\"\n",
    "gpt2_fresh.push_to_hub(repo_name, use_auth_token=True)\n",
    "gpt2_tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.2 (1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'meta-llama/Llama-3.2-1B-Instruct' with 1.24B parameters.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 3.2 1B \n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "llama32_1b = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "llama32_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded '{llama32_1b.config._name_or_path}' with {llama32_1b.num_parameters()/1e9:.2f}B parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, how are you? I'm a bit worried about the weather forecast.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=llama32_1b, tokenizer=llama32_tokenizer, pad_token_id=llama32_tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d3fc732e0b45d592708bb8bc654c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/llama32-1b-fresh\n"
     ]
    }
   ],
   "source": [
    "# Push fresh instance to HuggingFace Hub\n",
    "llama32_1b_fresh = LlamaForCausalLM(llama32_1b.config)\n",
    "llama32_1b_fresh.init_weights()\n",
    "\n",
    "# Push to HuggingFace Hub\n",
    "repo_name = \"llama32-1b-fresh\"\n",
    "llama32_1b_fresh.push_to_hub(repo_name, use_auth_token=True)\n",
    "llama32_tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to: https://huggingface.co/mikasenghaas/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_model, get_tokenizer\n",
    "from src.config import ModelConfig, TokenizerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'mikasenghaas/llama2-9m-fresh' with 9.77M parameters.\n",
      "Loaded 'mikasenghaas/llama2-9m-fresh' with 32000 tokens.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, how are you? accept accept System accept System accept System acceptäll пробле'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Llama 2 (9M)\n",
    "model_name = \"mikasenghaas/llama2-9m-fresh\"\n",
    "model = get_model(ModelConfig(name=model_name))\n",
    "tokenizer = get_tokenizer(TokenizerConfig(name=model_name))\n",
    "\n",
    "# Print model and tokenizer details\n",
    "print(f\"Loaded '{model.config._name_or_path}' with {model.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{tokenizer.name_or_path}' with {tokenizer.vocab_size} tokens.\\n\")\n",
    "\n",
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'mikasenghaas/gpt2-124m-fresh' with 124.44M parameters.\n",
      "Loaded 'mikasenghaas/gpt2-124m-fresh' with 50257 tokens.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, how are you? eb launches stylesoufl ffomach understoodrelevant PublishedYOU'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get GPT-2 124M\n",
    "model_name = \"mikasenghaas/gpt2-124m-fresh\"\n",
    "model = get_model(ModelConfig(name=model_name))\n",
    "tokenizer = get_tokenizer(TokenizerConfig(name=model_name))\n",
    "\n",
    "# Print model and tokenizer details\n",
    "print(f\"Loaded '{model.config._name_or_path}' with {model.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{tokenizer.name_or_path}' with {tokenizer.vocab_size} tokens.\\n\")\n",
    "\n",
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4e8702b92748d7bbdaabfe4f74a51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502752565d104edb9790dd1f7b7adef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'mikasenghaas/llama32-1b-fresh' with 1235.81M parameters.\n",
      "Loaded 'mikasenghaas/llama32-1b-fresh' with 128000 tokens.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, how are you? collapsesgridدید díl Frag wollen-out-economic činnosti Filename'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get LLama 3.2 1B\n",
    "model_name = \"mikasenghaas/llama32-1b-fresh\"\n",
    "model = get_model(ModelConfig(name=model_name))\n",
    "tokenizer = get_tokenizer(TokenizerConfig(name=model_name))\n",
    "\n",
    "# Print model and tokenizer details\n",
    "print(f\"Loaded '{model.config._name_or_path}' with {model.num_parameters()/1e6:.2f}M parameters.\")\n",
    "print(f\"Loaded '{tokenizer.name_or_path}' with {tokenizer.vocab_size} tokens.\\n\")\n",
    "\n",
    "# Generate text\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=\"cuda\")\n",
    "pipe(\"Hello, how are you?\", max_new_tokens=10)[0][\"generated_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
