{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Experiments\n",
    "\n",
    "This notebook shows how to train a model and load it from a checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Dict\n",
    "\n",
    "import wandb\n",
    "from wandb.sdk.wandb_run import Run\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WANDB_ENTITY = \"mikasenghaas\"\n",
    "WANDB_PROJECT = \"swarm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def get_config(run: Run) -> Dict:\n",
    "    return run.config\n",
    "\n",
    "def get_history(run: Run) -> pd.DataFrame:\n",
    "    run_id = run.id\n",
    "    history = run.history()\n",
    "    return pd.concat([pd.Series([run_id]*len(history), name=\"run_id\"), history], axis=1)\n",
    "\n",
    "def get_summary(run: Run) -> pd.Series:\n",
    "    return pd.DataFrame([dict(run.summary)], index=[run.id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get runs\n",
    "RUNS = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "print(f\"âœ… Loaded {len(RUNS)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Verify Gradient Accumulation\n",
    "\n",
    "This experiment verifies that gradient accumulation works as expected. We do so by training a model based on the debug configuration with different micro-batch sizes and the same global batch size locally (Apple M1).\n",
    "\n",
    "View the experiment: [W&B](https://wandb.ai/mikasenghaas/swarm/workspace?nw=dm6rh6z8t14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/grad-acc\"\n",
    "EXP1_RUNS = [r for r in RUNS if r.group == GROUP]\n",
    "\n",
    "print(f\"âœ… Loaded {len(EXP1_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP1_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP1_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP1_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss by step\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/current\", hue=\"run_id\", marker=\"o\", ax=ax[0])\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/average\", hue=\"run_id\", marker=\"o\", ax=ax[1])\n",
    "ax[0].set_title(\"Loss by Step\")\n",
    "ax[1].set_title(\"Loss by Step (Average)\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Step\")\n",
    "    a.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, gradient accumulation works. For every step, we are accumulating gradients over various micro-batches, and the we perform the same gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Wall-Time by Run\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.barplot(data=runs_summary, x=runs_summary.index, y=\"_runtime\", ax=ax[0])\n",
    "sns.barplot(data=runs_summary, x=runs_summary.index, y=\"train/throughput/average\", ax=ax[1])\n",
    "ax[0].set_title(\"Wall-Time by Run\")\n",
    "ax[1].set_title(\"Throughput by Run\")\n",
    "ax[0].set_ylabel(\"Wall-Time (s)\")\n",
    "ax[1].set_ylabel(\"Throughput (T/s)\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Micro-Batch Size\")\n",
    "    a.set_xticks(range(len(runs_summary)))\n",
    "    a.set_xticklabels([runs_config[run_id]['train']['micro_batch_size'] for run_id in runs_summary.index]);\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the wall-time decreases with increasing micro-batch size, as expected. This is, because we are processing more tokens per second (using GPU hardware more efficiently)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
