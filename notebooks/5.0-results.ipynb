{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Experiments\n",
    "\n",
    "This notebook shows how to train a model and load it from a checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Dict\n",
    "\n",
    "import wandb\n",
    "from wandb.sdk.wandb_run import Run\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WANDB_ENTITY = \"mikasenghaas\"\n",
    "WANDB_PROJECT = \"swarm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def get_config(run: Run) -> Dict:\n",
    "    return run.config\n",
    "\n",
    "def get_history(run: Run) -> pd.DataFrame:\n",
    "    run_id = run.id\n",
    "    history = run.history()\n",
    "    return pd.concat([pd.Series([run_id]*len(history), name=\"run_id\"), history], axis=1)\n",
    "\n",
    "def get_summary(run: Run) -> pd.Series:\n",
    "    return pd.DataFrame([dict(run.summary)], index=[run.id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get runs\n",
    "RUNS = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "print(f\"âœ… Loaded {len(RUNS)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Verify Gradient Accumulation\n",
    "\n",
    "This experiment verifies that gradient accumulation works as expected. We do so by training a model based on the debug configuration with different micro-batch sizes and the same global batch size locally (Apple M1).\n",
    "\n",
    "View the experiment: [W&B](https://wandb.ai/mikasenghaas/swarm/workspace?nw=dm6rh6z8t14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/grad-acc\"\n",
    "EXP1_RUNS = [r for r in RUNS if r.group == GROUP]\n",
    "\n",
    "print(f\"âœ… Loaded {len(EXP1_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP1_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP1_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP1_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss by step\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/current\", hue=\"run_id\", marker=\"o\", ax=ax[0])\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/average\", hue=\"run_id\", marker=\"o\", ax=ax[1])\n",
    "ax[0].set_title(\"Loss by Step\")\n",
    "ax[1].set_title(\"Loss by Step (Average)\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Step\")\n",
    "    a.set_ylabel(\"Loss\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, gradient accumulation works. For every step, we are accumulating gradients over various micro-batches, and the we perform the same gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Wall-Time by Run\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.barplot(data=runs_summary, x=runs_summary.index, y=\"_runtime\", ax=ax[0])\n",
    "sns.barplot(data=runs_summary, x=runs_summary.index, y=\"train/throughput/average\", ax=ax[1])\n",
    "ax[0].set_title(\"Wall-Time by Run\")\n",
    "ax[1].set_title(\"Throughput by Run\")\n",
    "ax[0].set_ylabel(\"Wall-Time (s)\")\n",
    "ax[1].set_ylabel(\"Throughput (T/s)\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Micro-Batch Size\")\n",
    "    a.set_xticks(range(len(runs_summary)))\n",
    "    a.set_xticklabels([runs_config[run_id]['train']['micro_batch_size'] for run_id in runs_summary.index]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the wall-time decreases with increasing micro-batch size, as expected. This is, because we are processing more tokens per second (using GPU hardware more efficiently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Cosine LR Scheduler\n",
    "\n",
    "This experiment verifies that the cosine learning rate scheduling works as expected, e.g. the learning rate is 0 at the start, then linearly increases for `train.scheduler.warmup_steps`, after which the cosine schedule kicks in and the learning rate decays according to a cosine annealing pattern until it reaches a minimum learning rate of `train.scheduler.min_lr_factor` of the initial learning rate. The experiment is run with the debug configuration from the script `experiments/verify/scheduler.sh` and run locally on an Apple M1.\n",
    "\n",
    "View the experiment: [W&B](https://wandb.ai/mikasenghaas/swarm/workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/scheduler\"\n",
    "EXP2_RUNS = [r for r in RUNS if r.group == GROUP]\n",
    "\n",
    "print(f\"âœ… Loaded {len(EXP2_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP2_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP2_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP2_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate patterns\n",
    "fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/learning_rate/current\", hue=\"run_id\", ax=ax)\n",
    "ax.set_title(\"Learning Rate by Step (All Runs)\")\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Learning Rate\")\n",
    "plt.legend(title=\"Run ID\")\n",
    "\n",
    "# Create custom legend with scheduler configuration\n",
    "run_ids = runs_config.keys()\n",
    "enable = [runs_config[run_id]['train']['scheduler']['enable'] for run_id in run_ids]\n",
    "warmup_steps = [runs_config[run_id]['train']['scheduler']['warmup_steps'] for run_id in run_ids]\n",
    "min_lr_factor = [runs_config[run_id]['train']['scheduler']['min_lr_factor'] for run_id in run_ids]\n",
    "\n",
    "legend_elements = []\n",
    "for run_id, e, w, m in zip(run_ids, enable, warmup_steps, min_lr_factor):\n",
    "    color = ax.get_lines()[list(run_ids).index(run_id)].get_color()\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=color, lw=2, label=f\"{run_id} (enable={e}, warmup_steps={w}, min_lr_factor={m})\"))\n",
    "\n",
    "ax.legend(handles=legend_elements, title=\"Scheduler Config\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, looks good. The hyperparameter affect the learning rate pattern as expected:\n",
    "\n",
    "- `enable`: The learning rate is constant at the initial learning rate for `False` and otherwise follows a cosine annealing pattern.\n",
    "- `warmup_steps`: The learning rate is linearly increased from the initial learning rate to the maximum learning rate over `warmup_steps` steps.\n",
    "- `min_lr_factor`: The learning rate is multiplied by `min_lr_factor` at the end of the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
