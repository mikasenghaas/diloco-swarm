{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Experiments\n",
    "\n",
    "This notebook analyses the results of experiments as tracked to W&B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Dict\n",
    "\n",
    "import wandb\n",
    "from wandb.sdk.wandb_run import Run\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WANDB_ENTITY = \"mikasenghaas\"\n",
    "WANDB_PROJECT = \"swarm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def get_gpu(run: Run) -> str:\n",
    "    if \"gpu_nvidia\" in run.metadata:\n",
    "        gpu = run.metadata[\"gpu_nvidia\"][0]\n",
    "        return {\"name\": gpu[\"name\"], \"memory\": gpu[\"memoryTotal\"], \"count\": len(run.metadata[\"gpu_nvidia\"])}\n",
    "    elif \"gpuapple\" in run.metadata:\n",
    "        return {\"name\": run.metadata[\"gpuapple\"][\"gpuType\"], \"count\": 1}\n",
    "    else:\n",
    "        return {\"name\": \"Unknown\"}\n",
    "\n",
    "def get_config(run: Run) -> Dict:\n",
    "    return {**run.config, \"gpu\": get_gpu(run)}\n",
    "\n",
    "def get_history(run: Run) -> pd.DataFrame:\n",
    "    run_id = run.id\n",
    "    history = run.history()\n",
    "    return pd.concat([pd.Series([run_id]*len(history), name=\"run_id\"), history], axis=1).set_index(\"run_id\")\n",
    "\n",
    "def get_summary(run: Run) -> pd.Series:\n",
    "    return pd.DataFrame([dict(run.summary)], index=[run.id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Styling\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette(\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get runs\n",
    "RUNS = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "print(f\"âœ… Loaded {len(RUNS)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Verify Gradient Accumulation\n",
    "\n",
    "This experiment verifies that gradient accumulation works as expected. We do so by training a model based on the debug configuration with different micro-batch sizes and the same global batch size locally (Apple M1).\n",
    "\n",
    "View the experiment: [W&B](https://wandb.ai/mikasenghaas/swarm/workspace?nw=dm6rh6z8t14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/grad-acc\"\n",
    "EXP1_RUNS = [r for r in RUNS if r.group == GROUP]\n",
    "\n",
    "print(f\"âœ… Loaded {len(EXP1_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP1_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP1_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP1_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss by step\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/current\", hue=\"run_id\", marker=\"o\", ax=ax[0])\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/average\", hue=\"run_id\", marker=\"o\", ax=ax[1])\n",
    "ax[0].set_title(\"Loss by Step\")\n",
    "ax[1].set_title(\"Loss by Step (Average)\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Step\")\n",
    "    a.set_ylabel(\"Loss\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, gradient accumulation works. For every step, we are accumulating gradients over various micro-batches, and the we perform the same gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Wall-Time by Run\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.barplot(data=runs_summary, x=runs_summary.index, y=\"_runtime\", ax=ax[0])\n",
    "sns.barplot(data=runs_summary, x=runs_summary.index, y=\"train/throughput/average\", ax=ax[1])\n",
    "ax[0].set_title(\"Wall-Time by Run\")\n",
    "ax[1].set_title(\"Throughput by Run\")\n",
    "ax[0].set_ylabel(\"Wall-Time (s)\")\n",
    "ax[1].set_ylabel(\"Throughput (T/s)\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Micro-Batch Size\")\n",
    "    a.set_xticks(range(len(runs_summary)))\n",
    "    a.set_xticklabels([runs_config[run_id]['train']['micro_batch_size'] for run_id in runs_summary.index]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the wall-time decreases with increasing micro-batch size, as expected. This is, because we are processing more tokens per second (using GPU hardware more efficiently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Cosine LR Scheduler\n",
    "\n",
    "This experiment verifies that the cosine learning rate scheduling works as expected, e.g. the learning rate is 0 at the start, then linearly increases for `train.scheduler.warmup_steps`, after which the cosine schedule kicks in and the learning rate decays according to a cosine annealing pattern until it reaches a minimum learning rate of `train.scheduler.min_lr_factor` of the initial learning rate. The experiment is run with the debug configuration from the script `experiments/verify/scheduler.sh` and run locally on an Apple M1.\n",
    "\n",
    "View the experiment: [W&B](https://wandb.ai/mikasenghaas/swarm/workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/scheduler\"\n",
    "EXP2_RUNS = [r for r in RUNS if r.group == GROUP]\n",
    "\n",
    "print(f\"âœ… Loaded {len(EXP2_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP2_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP2_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP2_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate patterns\n",
    "fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/learning_rate/current\", hue=\"run_id\", ax=ax)\n",
    "ax.set_title(\"Learning Rate by Step (All Runs)\")\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Learning Rate\")\n",
    "plt.legend(title=\"Run ID\")\n",
    "\n",
    "# Create custom legend with scheduler configuration\n",
    "run_ids = runs_config.keys()\n",
    "enable = [runs_config[run_id]['train']['scheduler']['enable'] for run_id in run_ids]\n",
    "warmup_steps = [runs_config[run_id]['train']['scheduler']['warmup_steps'] for run_id in run_ids]\n",
    "min_lr_factor = [runs_config[run_id]['train']['scheduler']['min_lr_factor'] for run_id in run_ids]\n",
    "\n",
    "legend_elements = []\n",
    "for run_id, e, w, m in zip(run_ids, enable, warmup_steps, min_lr_factor):\n",
    "    color = ax.get_lines()[list(run_ids).index(run_id)].get_color()\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=color, lw=2, label=f\"{run_id} (enable={e}, warmup_steps={w}, min_lr_factor={m})\"))\n",
    "\n",
    "ax.legend(handles=legend_elements, title=\"Scheduler Config\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, looks good. The hyperparameter affect the learning rate pattern as expected:\n",
    "\n",
    "- `enable`: The learning rate is constant at the initial learning rate for `False` and otherwise follows a cosine annealing pattern.\n",
    "- `warmup_steps`: The learning rate is linearly increased from the initial learning rate to the maximum learning rate over `warmup_steps` steps.\n",
    "- `min_lr_factor`: The learning rate is multiplied by `min_lr_factor` at the end of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Mixed Precision Training\n",
    "\n",
    "This experiment verifies that mixed precision training works as expected, i.e. that we can train a model with lower precision matrix multiplication and in `bfloat16` instead of `float32` without loosing too much performance. The experiment is run with the debug configuration from the script `experiments/verify/mp.sh` and run locally on an NVIDIA RTX 4090.\n",
    "\n",
    "View the experiment: [W&B]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/mp\"\n",
    "EXP3_RUNS = [r for r in RUNS if r.group == GROUP]\n",
    "\n",
    "print(f\"âœ… Loaded {len(EXP3_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP3_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP3_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP3_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss by step\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/current\", hue=\"run_id\", marker=\"o\", ax=ax[0])\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/loss/average\", hue=\"run_id\", marker=\"o\", ax=ax[1])\n",
    "\n",
    "# Create a custom legend\n",
    "legend_elements = []\n",
    "for run_id in runs_history.index.unique():\n",
    "    config = runs_config[run_id]\n",
    "    precision = config['train']['precision']\n",
    "    autocast = config['train']['autocast']\n",
    "    color = ax[0].get_lines()[list(runs_history.index.unique()).index(run_id)].get_color()\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=color, lw=2, label=f\"{run_id} (precision={precision}, autocast={autocast})\"))\n",
    "\n",
    "# Remove existing legends\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "\n",
    "# Add shared legend\n",
    "fig.legend(handles=legend_elements, title=\"Run Configuration\", loc=\"upper right\", bbox_to_anchor=(.99, .96))\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "ax[0].set_title('Loss')\n",
    "ax[1].set_title('Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss by step\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/throughput/current\", hue=\"run_id\", ax=ax[0])\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/throughput/average\", hue=\"run_id\", ax=ax[1])\n",
    "\n",
    "# Create a custom legend\n",
    "legend_elements = []\n",
    "for run_id in runs_history.index.unique():\n",
    "    config = runs_config[run_id]\n",
    "    precision = config['train']['precision']\n",
    "    autocast = config['train']['autocast']\n",
    "    color = ax[0].get_lines()[list(runs_history.index.unique()).index(run_id)].get_color()\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=color, lw=2, label=f\"{run_id} (precision={precision}, autocast={autocast})\"))\n",
    "\n",
    "# Remove existing legends\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "\n",
    "# Add shared legend\n",
    "fig.legend(handles=legend_elements, title=\"Run Configuration\", loc=\"lower right\", bbox_to_anchor=(0.99, .2))\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "ax[0].set_title('Throughput')\n",
    "ax[1].set_title('Average Throughput')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation are equivalent, which is weird. Given that we are varying the precision of the matrix multiplication, activations and gradients, there should be a difference. For now, the throughput is high enough, so we will only come back to this if the performance is an issue. At this point, I delete the `autocast` flag from the codebase, and re-run the `experiments/verify/mp.sh` script only for different values of internal precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/mp2\"\n",
    "EXP4_RUNS = [r for r in RUNS if r.group == GROUP]\n",
    "\n",
    "print(f\"âœ… Loaded {len(EXP4_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP4_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP4_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP4_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss by step\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 4), dpi=300)\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/throughput/current\", hue=\"run_id\", ax=ax[0])\n",
    "sns.lineplot(data=runs_history, x=\"_step\", y=\"train/throughput/average\", hue=\"run_id\", ax=ax[1])\n",
    "\n",
    "# Create a custom legend\n",
    "legend_elements = []\n",
    "for run_id in runs_history.index.unique():\n",
    "    config = runs_config[run_id]\n",
    "    precision = config['train']['precision']\n",
    "    color = ax[0].get_lines()[list(runs_history.index.unique()).index(run_id)].get_color()\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=color, lw=2, label=f\"{run_id} (precision={precision})\"))\n",
    "\n",
    "# Remove existing legends\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "\n",
    "# Add shared legend\n",
    "fig.legend(handles=legend_elements, title=\"Run Configuration\", loc=\"lower right\", bbox_to_anchor=(0.99, .2))\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "ax[0].set_title('Throughput')\n",
    "ax[1].set_title('Average Throughput')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for the `precision` flag set via `torch.set_float32_matmul_precision(precision)`, the performance is the same. Weird..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: GPU Benchmark\n",
    "\n",
    "In this experiment, we are benchmarking the performances of various GPUs on the [Prime Intellect Compute](https://api.primeintellect.ai) platform. Namely, we are comparing the following GPUs:\n",
    "\n",
    "- Apple ARM M1 (8GB)\n",
    "- NVIDIA RTX 4090 (24GB)\n",
    "- NVIDIA A100 (40GB)\n",
    "- NVIDIA A100 (80GB)\n",
    "- NVIDIA H100 (80GB)\n",
    "\n",
    "We are using the script `experiments/verify/perf.sh` to run the experiment. It uses the configuration from `configs/baseline/perf.yaml` and runs the `src/train/baseline.py` script. It trains the 14M Llama model on the entire train split of WikiText 2 (17.8M tokens), no intermediate evaluation but final test on WikiText 2 (2.2M tokens). We do not use learning rate scheduling and test for various micro batch sizes, starting from 1 up to 128 (or when reaching OOM).\n",
    "\n",
    "View the experiment: [W&B](https://wandb.ai/mikasenghaas/swarm/workspace?nw=5p39zizreht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs\n",
    "GROUP = \"verify/perf\"\n",
    "EXP5_RUNS = [r for r in RUNS if r.group == GROUP and \"hidden\" not in r.tags and r.state == \"finished\"]\n",
    " \n",
    "print(f\"âœ… Loaded {len(EXP5_RUNS)} runs for experiment {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config, summary, history\n",
    "runs_config = {r.id: get_config(r) for r in EXP5_RUNS}\n",
    "runs_summary = pd.concat([get_summary(r) for r in EXP5_RUNS])\n",
    "runs_history = pd.concat([get_history(r) for r in EXP5_RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct performance dataframe\n",
    "performance = runs_summary.copy()\n",
    "\n",
    "# Add GPU type to summary and history\n",
    "performance[\"gpu\"] = runs_summary.index.map(lambda x: runs_config[x][\"gpu\"][\"name\"])\n",
    "\n",
    "# Add micro-batch size to summary\n",
    "performance[\"micro_batch_size\"] = runs_summary.index.map(lambda x: str(runs_config[x][\"train\"][\"micro_batch_size\"]))\n",
    "\n",
    "# Add peak throughput to summary\n",
    "performance[\"train/throughput/max\"] = runs_summary.index.map(lambda x: runs_history[runs_history.index == x][\"train/throughput/current\"].max())\n",
    "\n",
    "performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average throughput per micro-batch size\n",
    "fig, ax = plt.subplots(figsize=(16, 4), dpi=300)\n",
    "stats = performance.groupby(\"gpu\")[\"train/throughput/average\"].describe().sort_values(by=\"mean\", ascending=False)\n",
    "sns.boxplot(data=performance, x=\"gpu\", y=\"train/throughput/average\", order=stats.index, ax=ax)\n",
    "ax.set_title(\"Average Throughput per GPU\")\n",
    "ax.set_xlabel(\"GPU\")\n",
    "ax.set_ylabel(\"Average Throughput (kT/s)\")\n",
    "ax.yaxis.set_major_formatter(lambda x, p: f'{x/1000:.0f}')\n",
    "plt.xticks(rotation=10, ha='center')\n",
    "plt.show();\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average throughput per micro-batch size and GPU\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(16, 8), dpi=300)\n",
    "fig.suptitle(\"Average Throughput per GPU and Micro-Batch Size\")\n",
    "stats = performance.groupby([\"gpu\", \"micro_batch_size\"])[\"train/throughput/average\"].mean()\n",
    "gpu_order = performance.groupby(\"gpu\")[\"train/throughput/average\"].mean().sort_values(ascending=True).index\n",
    "batch_size_order = [str(2**i) for i in range(7)]\n",
    "colors = sns.color_palette(\"Blues\", n_colors=7)\n",
    "sns.barplot(data=performance, x=\"gpu\", y=\"train/throughput/average\", hue=\"micro_batch_size\", order=gpu_order, hue_order=batch_size_order, ax=ax[0], gap=0.2, palette=colors)\n",
    "ax[0].set_xlabel(\"GPU\")\n",
    "ax[0].set_ylabel(\"Average Throughput (kT/s)\")\n",
    "ax[0].yaxis.set_major_formatter(lambda x, p: f'{x/1000:.0f}')\n",
    "ax[0].legend(title=\"Micro-Batch Size\")\n",
    "\n",
    "colors = sns.color_palette(\"Blues\", n_colors=5)\n",
    "sns.barplot(data=performance, x=\"micro_batch_size\", y=\"train/throughput/average\", hue=\"gpu\", order=batch_size_order, hue_order=gpu_order, ax=ax[1], gap=0.2, palette=colors)\n",
    "ax[1].set_xlabel(\"Micro-Batch Size\")\n",
    "ax[1].set_ylabel(\"Average Throughput (kT/s)\")\n",
    "ax[1].yaxis.set_major_formatter(lambda x, p: f'{x/1000:.0f}')\n",
    "ax[1].legend(title=\"GPU\");\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
