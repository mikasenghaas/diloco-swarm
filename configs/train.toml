[data]
seq_length = 1024

[train]
max_steps = -1 # ~1.900 steps for 1B tokens
max_epochs = 1
batch_size = 512
micro_batch_size = 1
max_norm = 1.0

[train.optimizer]
lr = 6e-4
decay = 0.1
betas = [0.9, 0.95]

[train.scheduler]
enable = true

[sample]
enable = true
every_n_steps = 10

[eval]
enable = true
max_steps = 10
every_n_steps = 10

[logging.console]
enable = true

[logging.wandb]
enable = true

[logging.file]
enable = true

[logging.ckpt]
enable = true
every_n_steps = 50
