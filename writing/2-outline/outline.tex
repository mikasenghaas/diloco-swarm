\documentclass[conference, 10pt]{IEEEtran}
\IEEEoverridecommandlockouts

% packages
\usepackage{parskip}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{footnote}
\usetikzlibrary{positioning, shapes, arrows.meta}

\begin{document}

\title{
  {\Large Research Outline}\\\vspace{1pt}
  {\LARGE Battle-Testing SWARM: Decentralised Learning on Volunteer Compute}
}

\author{
  \IEEEauthorblockN{\bf Mika Senghaas\IEEEauthorrefmark{1}}
  \IEEEauthorblockA{EPFL \\ \textit{mika.senghaas@epfl.ch}}  
  \and
  \IEEEauthorblockN{\bf Martijn De Vos\IEEEauthorrefmark{2}}
  \IEEEauthorblockA{EPFL \\ \textit{martijn.devos@epfl.ch}}
  \and
  \IEEEauthorblockN{\bf Rishi Sharma\IEEEauthorrefmark{2}}
  \IEEEauthorblockA{EPFL \\ \textit{rishi.sharma@epfl.ch}}
  \and
  \IEEEauthorblockN{\bf Akash Dhasad\IEEEauthorrefmark{2}}
  \IEEEauthorblockA{EPFL \\ \textit{akash.dhasade@epfl.ch}}
}

\maketitle

{\small\itshape \IEEEauthorrefmark{1} Author, \IEEEauthorrefmark{2} Supervisor}

\begin{abstract} 
  This document outlines the research plan for battle-testing
  SWARM~\cite{ryabinin2023}, a decentralised learning algorithm, on
  heterogeneous volunteer compute. The project is developed in collaboration
  with the Scalable Computing Systems (SaCS) lab at EPFL and Prime Intellect.
\end{abstract}

\begin{IEEEkeywords}
Deep Learning, Decentralised Training, Pipeline Parallelism, SWARM
\end{IEEEkeywords}

\section{Introduction} 

% Scale
Following the promises of the scaling laws~\cite{kaplan2020,hoffmann2022},
modern foundation models have hundreds of billion of parameters and are trained
on tens of trillions of
tokens~\cite{brown2023,touvron2023,dubey2024,openai2024,chowdhery2022,geminiteam2024}.
At this scale, it is infeasible to train on a single device. A natural solution
is to distribute computation, i.e. have many devices collaboratively train a
single model.

% SOTA Distributed training
The main idea in distributed training is to partition the data, model, and
optimiser states across devices to lower the memory usage per device and
training time through parallel computation. Many parallelisation techniques have
been proposed, most notably data parallelism, pipeline parallelism , and tensor
parallelism. Often, models use these techniques in conjunction~\cite{dubey2024,
shoeybi2020}. For example, Llama 3.1 405B employs four types of parallelism
across 16K H100 GPUs~\cite{dubey2024}. However, naive implementations of these
techniques only work well under the assumption of fast, homogeneous compute, as
present in data centres.

% Problem and promise
Training at this scale in data centres is expensive and only feasible for a
selected few companies with the necessary resources. Thus, the current paradigm
limits who can participate in AI development and slows the pace of innovation.

% Decentralisation
Decentralisation is a promising alternative to data centre training. If we had
efficient algorithms for large-scale pre-training on heterogeneous devices and
networks, we could use the vast cheap, consumer-grade resources across the globe
to train the next generation of open-source foundation
models~\cite{weisser2024}. However, training decentralised is challenging:

\begin{enumerate}[label=(\arabic*)]
  \item (\textit{Network}) Devices may be distributed globally; an efficient
    algorithm trains efficiently despite slow interconnect
  \item (\textit{Hardware}) Devices may range from consumer-grade to
    data centre GPUs; an efficient algorithm  ensures high utilisation across
    all devices
  \item (\textit{Reliability}) Devices may leave or enter the training at any
    time; a robust algorithm is fault-tolerant and dynamically adapts to the
    amount of compute available
  \item \textit{(Scale)} Devices may be numerous; a scalable method should be
    more efficient as more compute becomes available, independent of the model or
    data size
\end{enumerate}

This work considers the most challenging environment for decentralised learning:
leveraging volunteer compute, i.e. potentially unreliable, consumer-grade
devices with variable connectivity and capabilities, to train large-scale
language models. A promising candidate for this setting is
SWARM~\cite{ryabinin2023}, a dynamic pipeline parallelism approach with infinite
horizontal scalability. An inference variant of SWARM, called
Petals~\cite{borzunov2023}, is the current state-of-the-art for decentralised 
inference. While promising on paper, the original SWARM algorithm has only been
tested in a restricted setting. This work aims to \textit{battle-test} SWARM by
empirically evaluating its performance in diverse configurations of compute
ensembles.

% TODO: Go into more detail on the a) settings, and b) performance metrics

\section{Background}
\label{sec:background}

% Data parallelism
Stochastic gradient-based training naturally lends itself to an easy form of
parallelism, called data parallelism. Here, each worker holds a copy of the
entire model and a local data shard. Each device trains locally, before the local
gradients are synchronised. Traditional data parallel methods~\cite{mcmahan2016}
require synchronisation of local gradients at every step, rendering the method
unfeasible in a decentralised setting as communication costs are too high.

% DiLoCo
DiLoCo~\cite{douillard2023} is a framework that enables low-bandwidth data
parallel training on poorly connected islands of workers. It proposes an
inner-outer optimisation scheme, in which workers train for $H\sim 500$ steps on
local data shards using AdamW~\cite{loshchilov2019} before synchronising
pseudo-gradients using a global Nesterov optimiser. DiLoCo sets the standard for
distributed data parallel training, matching performance of traditional methods
while significantly reducing wall clock-time and communication cost. While
DiLoCo is robust to dynamic worker arrangements, it faces many of the same
challenges as traditional data parallel training. In particular, the synchronous
nature of the global gradient update can lead to significant idle time when
faster workers have to wait for slower workers to finish their local training
task. Further, DiLoCo does not scale naturally to huge models that do not fit
into the workers' memory - relying on slow parameter offloading
methods~\cite{rhu2016, cui2016} to scale.

% Asynchronicity in DiLoCo
Follow up work from DeepMind~\cite{li2024} tries to address the efficiency
issues in heterogeneous environments by allowing asynchronous global updates on
a centralised parameter server. While comprehensive, the work does not find a
scalable solution to asynchronous data parallel training yet.

% TODO: Write more about this

% Pipeline parallelism
When a single worker cannot fit the entire model into memory, the model itself
has to be sharded. A common technique is pipeline parallelism which partitions a
model vertically into groups of subsequent layers. In this paradigm, activations
need to be communicated between workers handling subsequent layers during the
forward, and gradients during the backward pass. Traditional pipeline
parallelism is not feasible in the decentralised setting. Due to its inherently
sequential nature, the entire pipeline is bottlenecked by its weakest link,
leading to idle time for quicker workers. Furthermore, a naive implementation is
not fault-tolerant, as the failure of any worker will stall the entire training
procedure.

% SWARM
SWARM parallelism~\cite{ryabinin2023} translates the idea of pipeline
parallelism into an efficient algorithm in the distributed setting. Their main
innovation is in constructing a dynamic and redundant pipeline: each pipeline
stage is served by a swarm of devices. Any device in a swarm may communicate
with any device in the preceding and succeeding stage. They introduce
\textit{stochastic wiring} where each device makes a request to the next
pipeline stage according to statistics on the throughput of the available
devices, ensuring that faster or better connected devices handle more requests.
Further, due to \textit{swarm balancing}, devices periodically switch from
under- to overutilised swarms to distribute the workload. In theory, any worker
that can hold a (small) shard of the model in memory can participate in SWARM. 
High latency and high dropout rates in the network increase training time, but 
stochastic wiring and adaptive swarming helps to find optimal trajectories
through the pipeline stage, even under suboptimal conditions. SWARM naturally
satisfies on- and off-ramping of devices during training, and the algorithm is
naturally fault-tolerant due to its redundant design. In summary, SWARM was
designed from first principles to be robust and effective in the decentralised
setting considered in this work.

% Genetic scheduling
Finally, Yuan et. al~\cite{yuan2022} try to combine data and pipeline
parallelism by finding an optimal allocation of devices to micro-batches and
pipeline stages such that the global communication requirements are minimised.
They propose a genetic algorithm that jointly minimises communication
requirements. Yet, their global scheduling is static and, consequently, does not
handle device failures.

% Complementary research directions
Many works also consider complementary approaches to decrease communication
requirements, for example by gradient sparsification, quantisation or delayed
parameter updates~\cite{yuan2022, ryabinin2023, tang2020}.

\section{Method}
\label{sec:method}

% SWARM parallelism~\cite{ryabinin2023} is naturally
% closest to the problem setting considered for this work. However, in the
% original work the authors only provide empirical validation for relatively small
% models trained on spot instances. This work could (re-)implement SWARM and
% benchmark it in various settings, with the ultimate goal of training a
% larger-scale, e.g. 10B+ parameter model on volunteer compute.


\subsection{SWARM}
\label{subsec:swarm}

% Detail algorithm
% How? Tools to implement? Existing implmentations? Frameworks?

\subsection{Task}
\label{subsec:task}

% Detail learning task
% How? PyTorch + Hugging Face

\subsection{Evaluation}
\label{subsec:evaluation}

% Detail evaluation setup

\textit{-- Lausanne, 01.10.2024}

\bibliography{references}
\bibliographystyle{IEEEtran}

\end{document}
