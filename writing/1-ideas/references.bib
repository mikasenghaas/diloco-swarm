@misc{kaplan2020,
  title = {{Scaling Laws for Neural Language Models}},
  author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown
            and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford
            and Jeffrey Wu and Dario Amodei},
  year = {2020},
  eprint = {2001.08361},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2001.08361},
}

@misc{hoffmann2022,
  title = {{Training Compute-Optimal Large Language Models}},
  author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena
            Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las
            Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and
            Tom Hennigan and Eric Noland and Katie Millican and George van den
            Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and
            Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and
            Laurent Sifre},
  year = {2022},
  eprint = {2203.15556},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2203.15556},
}

@misc{vaswani2017,
  title = {{Attention Is All You Need}},
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit
            and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia
            Polosukhin},
  year = {2023},
  eprint = {1706.03762},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1706.03762},
}

@misc{brown2023,
  title = {{Language Models are Few-Shot Learners}},
  author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah
            and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and
            Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal
            and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and
            Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu
            and Clemens Winter and Christopher Hesse and Mark Chen and Eric
            Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack
            Clark and Christopher Berner and Sam McCandlish and Alec Radford and
            Ilya Sutskever and Dario Amodei},
  year = {2020},
  eprint = {2005.14165},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2005.14165},
}

@misc{openai2024,
      title={{GPT-4 Technical Report}},
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{shoeybi2020,
  title = {{Megatron-LM: Training Multi-Billion Parameter Language Models Using
           Model Parallelism}},
  author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick
            LeGresley and Jared Casper and Bryan Catanzaro},
  year = {2020},
  eprint = {1909.08053},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1909.08053},
}

@misc{touvron2023,
      title={{LLaMA: Open and Efficient Foundation Language Models}}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{dubey2024,
  title = {{The Llama 3 Herd of Models}},
  author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and \dots},
  year = {2024},
  eprint = {2407.21783},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2407.21783},
}

@misc{chowdhery2022,
      title={{PaLM: Scaling Language Modeling with Pathways}},
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.02311}, 
}

@misc{geminiteam2024,
      title={{Gemini: A Family of Highly Capable Multimodal Models}}, 
      author={Gemini Team},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{weisser2024,
  title = {{Introducing Prime Intellect}},
  howpublished = {\url{
                  https://www.primeintellect.ai/blog/introducing-prime-intellect}
                  },
  note = {Accessed: 2024-09-24},
}

@misc{loshchilov2019,
  title = {{Decoupled Weight Decay Regularization}},
  author = {Ilya Loshchilov and Frank Hutter},
  year = {2019},
  eprint = {1711.05101},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/1711.05101},
}

@misc{mcmahan2016,
  title = {{Communication-Efficient Learning of Deep Networks from Decentralized
           Data}},
  author = {H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth
            Hampson and Blaise Agüera y Arcas},
  year = {2023},
  eprint = {1602.05629},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/1602.05629},
}

@misc{yuan2022,
  title = {{Decentralized Training of Foundation Models in Heterogeneous
           Environments}},
  author = {Binhang Yuan and Yongjun He and Jared Quincy Davis and Tianyi Zhang
            and Tri Dao and Beidi Chen and Percy Liang and Christopher Re and Ce
            Zhang},
  year = {2023},
  eprint = {2206.01288},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  url = {https://arxiv.org/abs/2206.01288},
}

@misc{douillard2023,
  title = {{DiLoCo: Distributed Low-Communication Training of Language Models}},
  author = {Arthur Douillard and Qixuan Feng and Andrei A. Rusu and Rachita
            Chhaparia and Yani Donchev and Adhiguna Kuncoro and Marc'Aurelio
            Ranzato and Arthur Szlam and Jiajun Shen},
  year = {2023},
  eprint = {2311.08105},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2311.08105},
}

@misc{jaghouar2024,
  title = {{OpenDiLoCo: An Open-Source Framework for Globally Distributed
           Low-Communication Training}},
  author = {Sami Jaghouar and Jack Min Ong and Johannes Hagemann},
  year = {2024},
  eprint = {2407.07852},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2407.07852},
}

@misc{li2024,
  title = {Asynchronous Local-SGD Training for Language Modeling},
  author = {Bo Liu and Rachita Chhaparia and Arthur Douillard and Satyen Kale
            and Andrei A. Rusu and Jiajun Shen and Arthur Szlam and Marc'Aurelio
            Ranzato},
  year = {2024},
  eprint = {2401.09135},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2401.09135},
}

@misc{ryabinin2023,
  title = {{SWARM Parallelism: Training Large Models Can Be Surprisingly
           Communication-Efficient}},
  author = {Max Ryabinin and Tim Dettmers and Michael Diskin and Alexander
            Borzunov},
  year = {2023},
  eprint = {2301.11913},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  url = {https://arxiv.org/abs/2301.11913},
}

@inproceedings{borzunov2023,
  title = {{Petals: Collaborative Inference and Fine-tuning of Large Models}},
  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and
            Riabinin, Maksim and Belkada, Younes and Chumachenko, Artem and
            Samygin, Pavel and Raffel, Colin},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 3: System Demonstrations)},
  pages = {558--568},
  year = {2023},
  url = {https://arxiv.org/abs/2209.01188},
}

@misc{tang2020,
  title = {{Communication-Efficient Decentralized Learning with Sparsification
           and Adaptive Peer Selection}},
  author = {Zhenheng Tang and Shaohuai Shi and Xiaowen Chu},
  year = {2020},
  eprint = {2002.09692},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2002.09692},
}

@inproceedings{cui2016,
author = {Cui, Henggang and Zhang, Hao and Ganger, Gregory R. and Gibbons, Phillip B. and Xing, Eric P.},
title = {GeePS: scalable deep learning on distributed GPUs with a GPU-specialized parameter server},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901323},
doi = {10.1145/2901318.2901323},
abstract = {Large-scale deep learning requires huge computational resources to train a multi-layer neural network. Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections. While the computation involved can be done more efficiently on GPUs than on more traditional CPU cores, training such networks on a single GPU is too slow and training on distributed GPUs can be inefficient, due to data movement overheads, GPU stalls, and limited GPU memory. This paper describes a new parameter server, called GeePS, that supports scalable deep learning across GPUs distributed among multiple machines, overcoming these obstacles. We show that GeePS enables a state-of-the-art single-node GPU implementation to scale well, such as to 13 times the number of training images processed per second on 16 machines (relative to the original optimized single-node code). Moreover, GeePS achieves a higher training throughput with just four GPU machines than that a state-of-the-art CPU-only system achieves with 108 machines.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {4},
numpages = {16},
location = {London, United Kingdom},
series = {EuroSys '16}
}

@misc{rhu2016,
      title={{vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design}}, 
      author={Minsoo Rhu and Natalia Gimelshein and Jason Clemons and Arslan Zulfiqar and Stephen W. Keckler},
      year={2016},
      eprint={1602.08124},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1602.08124}, 
}